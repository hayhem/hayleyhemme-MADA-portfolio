---
title: "Tidy Tuesday Exercise 2"
output: 
  html_document:
    toc: FALSE
---
Let's load the data and other packages. We'll use the tidytuesdayR package to do this. 
```{r}
library(tidyverse)
library(lubridate)
library(tidymodels)
library(tidytuesdayR)
tuesdata <- tidytuesdayR::tt_load('2023-04-11')

eggproduction <- tuesdata$`egg-production`
cagefreepercentages <- tuesdata$`cage-free-percentages`
```

I've copied the following data dictionary from the TiduTuesday GitHub to reference more easily. 

### Data Dictionary

# `egg-production.csv`

|variable       |class     |description    |
|:--------------|:---------|:--------------|
|observed_month |double    |Month in which report observations are collected,Dates are recorded in ISO 8601 format YYYY-MM-DD |
|prod_type      |character |type of egg product: hatching, table eggs      |
|prod_process   |character |type of production process and housing: cage-free (organic), cage-free (non-organic), all. The value 'all' includes cage-free and conventional housing.   |
|n_hens         |double    |number of eggs produced by hens for a given month-type-process combo   |
|n_eggs         |double    |number of hens producing eggs for a given month-type-process combo     |
|source         |character |Original USDA report from which data are sourced. Values correspond to titles of PDF reports. Date of report is included in title.   |


# `cage-free-percentages.csv`

|variable       |class     |description    |
|:--------------|:---------|:--------------|
|observed_month |double    |Month in which report observations are collected,Dates are recorded in ISO 8601 format YYYY-MM-DD |
|percent_hens   |double    |observed or computed percentage of cage-free hens relative to all table-egg-laying hens  |
|percent_eggs   |double    |computed percentage of cage-free eggs relative to all table eggs,This variable is not available for data sourced from the Egg Markets Overview report |
|source         |character |Original USDA report from which data are sourced. Values correspond to titles of PDF reports. Date of report is included in title.  |



Let's start be taking a look at the data
```{r}
glimpse(eggproduction)
glimpse(cagefreepercentages)
```

It seems like these datasets span a different range of years. Let's verify this.
```{r}
eggproduction %>% 
  arrange(observed_month) %>%
  slice(c(1, n())) %>%
  bind_rows()

cagefreepercentages %>% 
  arrange(observed_month) %>%
  slice(c(1, n())) %>%
  bind_rows()
```
The cage-free percentages data goes back to 2007, while the egg production data only goes back to 2016. The most recent observations in both datasets is Feb. 28, 2021. 

My data with the hypothesis that the number of eggs produced per hen differs based on the type of egg product (hatchling or table eggs). 

H0: There is no difference between the number of eggs produced per hen.
HA: There is a difference between the number of eggs produced per hen, and table eggs are produced at higher rates compared to hatchling eggs.

The outcome interest is the rate of egg production, which I will name `rate_prod` for rate of production. 
```{r}
eggproduction <- eggproduction %>% 
  group_by(prod_type) %>% 
  mutate(rate_prod  = n_eggs/n_hens)
```

Let's prepare the datasets for a merge, which will hopefully this will make things easy to work with. I'll start by making a variable that pulls just the month and year from 'observed_month' to make merge the datasets more seamless.
```{r}
cagefreepercentages <- cagefreepercentages %>% 
  mutate(mon_yr = format_ISO8601(observed_month, precision = "ym"),
         mon = format(observed_month, "%m"),
        mon = as.factor(mon)) #only month

eggproduction <- eggproduction %>% 
  mutate(mon_yr = format_ISO8601(observed_month, precision = "ym"), 
         mon = format(observed_month, "%m"),
        mon = as.factor(mon))
         
```

I'll do a bit more tidying up by using the computed estimates and by removing unnecessary variables
```{r}
cagefreepercentages_clean <- cagefreepercentages %>% 
  filter(source == "computed")%>% 
  select(-c(observed_month, source))

eggproduction_clean <- eggproduction %>% 
  select(-c(observed_month, source))
```

Now, we'll use inner join to merge by the mon_yr only that appear in both datasets
```{r}
eggs_df <- inner_join(eggproduction_clean, cagefreepercentages_clean, by = join_by(mon_yr, mon))

eggs_df <- eggs_df %>% 
  mutate(prod_type = as.factor(prod_type),
         prod_process = as.factor(prod_process))

eggs_df$prod_type <- relevel(eggs_df$prod_type, ref = "table eggs")

#eggs_clean <- eggs_df %>% select(-mon_yr)
```

Let's split the data into test and train
```{r}
## setting the seed 
set.seed(123)
## Put 3/4 of the data into the training set 
data_split <- initial_split(eggs_df, prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data <- testing(data_split)

egg_metrics <- metric_set(accuracy, roc_auc, mn_log_loss)

#Cross validation
set.seed(123)
five_fold <- vfold_cv(train_data, v = 5, strata = prod_type)
```

Set the recipe
```{r}
eggs_rec <- recipe(prod_type ~ prod_process + mon + rate_prod, data = train_data) %>% 
  step_dummy(prod_process, mon)
```

Now let's set up a null model
```{r}
null_mod <- null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("classification")
```

Null workflow
```{r}
null_wflow <- workflow() %>%
  add_model(null_mod) %>%
  add_recipe(eggs_rec)
```

```{r}
null_fit <- null_wflow %>% 
  fit(data = train_data)

null_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```

Decision Tree
```{r}
library(rpart)

tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
```

```{r warning=FALSE}
## workflow for decision tree
tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(eggs_rec)

tree_res <- 
  tree_wf %>% 
  tune_grid(resamples = five_fold,
    grid = tree_grid) 
```

```{r}
tree_res %>% 
  collect_metrics() 

#visualize
tree_res %>%
  autoplot()

#Selecting the best tree using rmse.
best_tree <- tree_res %>%
  select_best(metric = "roc_auc") 
```
I am unsure if I am interpretting this correctly, but it seems like accuracy and ROC_AUC were perfect?

Logistic regression
```{r}
log_spec <- logistic_reg() %>%
  set_engine(engine = "glm") %>%
  set_mode("classification")
```

Creating workflow object for logistic regression
```{r}
log_wflow <- 
   workflow() %>% 
   add_recipe(eggs_rec) %>% 
   add_model(log_spec)
```

Creating trained workflow
```{r}
logreg_fit <- log_wflow %>% 
  fit(data = train_data)
```

Pulling model fit
```{r}
logreg_fit %>% 
  extract_fit_parsnip()  
```

Random forest
```{r}
library(ranger)

rf_spec <- 
  rand_forest(min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")
```

Random forest workflow
```{r}
rf_wflow <- workflow() %>%
 add_recipe(eggs_rec) %>% 
 add_model(rf_spec) 
```

Fit a model
```{r}
rf_wflow_fit <- rf_wflow %>% 
  fit(data = train_data)
```

```{r}
doParallel::registerDoParallel()

set.seed(345)
tune_res <- tune_grid(
  rf_wflow,
  resamples = five_fold,
  grid = 20)

#Visualize..
tune_res %>% 
  autoplot()

best_rf <-   tune_res %>% 
  select_best(metric = "roc_auc")

rf_final_wf <- rf_wflow %>%
  finalize_workflow(best_rf)

rf_final_wf %>% 
  fit(data = train_data)
```
Same results as above...

Multinomial regression via neural net
```{r}
# Specify a multinomial regression via nnet
multireg_spec <- multinom_reg(penalty = 1) %>% 
  set_engine("nnet") %>% 
  set_mode("classification")

#trying to set workflow
multireg_wf <- workflow() %>% 
  add_model(multireg_spec) %>% 
  add_recipe(eggs_rec)

# Train a multinomial regression model ##
set.seed(2056)
multireg_fit <- multireg_spec %>% 
  fit(prod_type ~ prod_process + mon + rate_prod, data = train_data)
  
#print model
multireg_fit %>%
  tidy()
```

All the models appears to perform similarly... the logistic regression model is the might be the simplest model, so let's use that to to test model performance against the test_data
```{r}
# Make predictions for the test set
eggs_results <- test_data %>% 
  select(prod_type) %>% 
  bind_cols(logreg_fit %>% 
              predict(new_data = test_data)) %>% 
  bind_cols(logreg_fit %>% 
              predict(new_data = test_data, type = "prob"))

# Print predictions
eggs_results %>% 
  slice_head(n = 54)
```

```{r}
#Confusion matrix
conf_mat(eggs_results,
         truth = prod_type,
         estimate = .pred_class) 
## Did not work like expected...

#pulling accuracy
accuracy(eggs_results, 
         truth = prod_type,
         estimate = .pred_class)
##Is 100% accurate

## trying to pull ROC-AUC to see if performance of predictive model is good
roc_auc(eggs_results,
        truth = prod_type,
        `.pred_hatching eggs`) 
##estimate is NA?
```
As far as I can tell, it seemed like the logistic regression model was able to predict prod_type perfectly. Intuitively, it seems like these models should not performed perfectly, but in looking at the data it is very clear that hens producing table eggs lay significantly more eggs than those producing hatching eggs. Unfortunately, my knowledge of tidymodels has limited my ability to troubleshoot potential issues in these classification models...
