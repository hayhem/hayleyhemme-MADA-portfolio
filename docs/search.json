[
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "My name is Hayley Hemme and I’m a second-year master of public health student concentrating in epidemiology. I received my bachelor of science in Biology from Brenau University in 2017 where I concentrated in biomedical sciences. I have previous training as an HIV tester and counselor, and worked in the emergency department as a medical scribe and clinical technician before returning to school to pursue my masters.\nMy primary interest is in infectious disease epidemiology, specifically HIV. An area of research I am interested in is the use of molecular HIV surveillance to detect clusters, and how these methods might be used with conjunction with spatial methods to predict future clusters and help better inform resource allocation.\nI have taken multiple courses that utilize statistics, programming, and/or data analysis, including all the required core classes for my program, as well as Spatial Epidemiology and Introduction to Coding in R. I experience working with STATA, SAS, and R, and while I am most comfortable working in R, I know there is still a lot for me to learn! What I hope to get most out of this course is functional knowledge of variety of data analysis methods and to improve on communicating results to stakeholders from a variety of backgrounds.\n\nAn interesting fact about me\n\nI am an intermediate-level aerialist training on trapeze and aerial sling.\nI am exotic pet hobbyist with a fear of “bugs” and arachnids, despite having several in my collection.\nI enjoy doing tricks training with my dog, Punch.\n\n\n\n\nMe and my dog Punch at Boo-le-bark.\n\n\n\n\nMyth: The Data Speaks for Itself\nThe link above is to a post on blog that was recently shared with me. I appreciate the author’s insight to data reporting, however blunt they may be. This particular post speaks to the perceived objectivity of data. I think that it’s important to understand that data may be interpreted in different ways, even among those with data literacy. She advocates that data story-telling be presented alongside results of data in order to better explain real-world implications."
  },
  {
    "objectID": "coding_exercise.html",
    "href": "coding_exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Loading packages\nLoading data\nSubsetting data from African countries\nCreating new objects from ‘africadata’– ‘imle’ containing infant mortality and life expectancy, and ‘ple’ containing population size and life expectancy\nData Visualization\nNext, let’s find out which years are missing data on infant mortality\nSubsetting data from the year 2000\nPlotting data from African Countries in 2000\nFitting a simple model\nConclusions\nThere is statistically significant evidence supporting an association between life expectancy and infant mortality. Life expectancy decreases by 2.49 years for every unit increase in infant mortality (p < 0.05). The data does not show statistically significant evidence of association between life expectancy and population size (p = 0.62)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hayley Hemme’s Website and Data Analysis Portfolio",
    "section": "",
    "text": "Hello!\n\nWelcome to my website and data analysis portfolio.\nPlease check back throughout the Spring 2023 semester to stay up to date with my progress in the MADA course.\n\nPlease use the Menu Bar above to look around.\nThank you!"
  },
  {
    "objectID": "tidytuesday_exercise.html",
    "href": "tidytuesday_exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "library(here)\nlibrary(epiDisplay)\nlibrary(tidyverse)\nlibrary(plotly)\n\nThis data was obtained from the github Importing the data\n\nage_gaps <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-14/age_gaps.csv')\n\nwrite_csv(age_gaps, here(\"data/age_gaps.csv\"))\n\nLet’s start by taking a glimpse at the dataset.\n\nglimpse(age_gaps)\n\nRows: 1,155\nColumns: 13\n$ movie_name         <chr> \"Harold and Maude\", \"Venus\", \"The Quiet American\", …\n$ release_year       <dbl> 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 199…\n$ director           <chr> \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joe…\n$ age_difference     <dbl> 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35,…\n$ couple_number      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ actor_1_name       <chr> \"Ruth Gordon\", \"Peter O'Toole\", \"Michael Caine\", \"D…\n$ actor_2_name       <chr> \"Bud Cort\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", \"T…\n$ character_1_gender <chr> \"woman\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", …\n$ character_2_gender <chr> \"man\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", \"…\n$ actor_1_birthdate  <date> 1896-10-30, 1932-08-02, 1933-03-14, 1930-09-17, 19…\n$ actor_2_birthdate  <date> 1948-03-29, 1982-06-03, 1982-10-01, 1975-11-08, 19…\n$ actor_1_age        <dbl> 75, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65,…\n$ actor_2_age        <dbl> 23, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30,…\n\n\nLet’s take a look at some summary statistics on age_difference.\n\nage_gaps %>% pull(age_difference) %>% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    4.00    8.00   10.42   15.00   52.00 \n\n\nLet’s see how many unique movies are included in the dataset.\n\nage_gaps %>% pull(movie_name) %>% n_distinct()\n\n[1] 830\n\n\nThere are 830 unique movies.\nLet’s also look at how many different directors appear in the dataset.\n\nage_gaps %>% pull(director) %>% n_distinct()\n\n[1] 510\n\n\nThere are 510 unique directors.\nLet’s make a new variable for th the number of movies released each decade in the dataset. We’ll do this by performing integer division using %/% and multiplying by 10.\n\nage_gaps <- age_gaps %>% \n  mutate(decade_released = 10 * (release_year %/% 10))\n\n\nage_gaps %>% \n  count(decade_released) %>% \n  ggplot() + geom_line(aes(decade_released, n)) +\n  scale_x_continuous(breaks = seq(1930,2020, 10))\n\n\n\n\nWe can see that the decade with the most movie releases was 2000.\nLet’s create variable for the largest age difference by decade released.\n\nage_gaps <- age_gaps %>%\n  group_by(decade_released) %>% \n  mutate(max_age_diff = max(age_difference))\n\nage_gaps %>% \n   ggplot() + \n  geom_line(data = age_gaps, aes(x = decade_released, y= max_age_diff))  + \n  scale_x_continuous(breaks = seq(1930,2020, 10))\n\n\n\n\nLet’s see if we can combine these plots and add some design elements.\n\nplot <- age_gaps %>% \n  count(decade_released) %>% \n  ggplot() + geom_line(aes(decade_released, n), color = \"red\", linewidth = 1) +\n    geom_line(data = age_gaps, aes(x = decade_released, y= max_age_diff),\n              color = \"pink\" , linewidth = 1)  +\n  scale_x_continuous(breaks = seq(1930,2020, 10)) + \n  labs(x = \"Year of Movie Release\", y = \"Number of Movies Released\",\n       title = \"Number of Movies Released with Love Interests and Largest Age Gap,\n       by Decade\") +\n  theme_classic()\n\nplot \n\n\n\n\nLet’s make the plot interactive!\n\nremove_buttons <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',\n                              'zoomOut2d','autoScale2d','hoverClosestCartesian',\n                              'toggleSpikelines','hoverCompareCartesian')\n\nggplotly(plot) %>% config(modeBarButtonsToRemove = remove_buttons)  \n\n\n\n\n\nNext, let’s switch gears entirely and find out what the most common name is among directors. We’ll need to load tidytext().\n\nlibrary(tidytext)\n\nage_gaps %>% ungroup() %>%\n  distinct(director) %>% \n  unnest_tokens(word, director) %>% \n  count(word, sort = TRUE) %>% \n  top_n(15)\n\n# A tibble: 15 × 2\n   word        n\n   <chr>   <int>\n 1 john       17\n 2 david      14\n 3 michael    13\n 4 peter      12\n 5 paul       11\n 6 george     10\n 7 james      10\n 8 robert     10\n 9 richard     7\n10 jon         6\n11 kevin       6\n12 lee         6\n13 mark        6\n14 scott       6\n15 steven      6\n\n\nLet’s see what the most common names are among directors with a ‘large’ age difference. Let’s use observations with an age difference >= 15 years, which is >= 75th quantile.\n\nage_gaps %>% pull(age_difference) %>% quantile()\n\n  0%  25%  50%  75% 100% \n   0    4    8   15   52 \n\nage_gaps %>% ungroup() %>%\n  filter(age_difference >= 15) %>% \n  distinct(director) %>% \n  unnest_tokens(word, director) %>% \n  count(word, sort = TRUE) %>%\n  top_n(15)\n\nSelecting by n\n\n\n# A tibble: 16 × 2\n   word        n\n   <chr>   <int>\n 1 john        8\n 2 david       6\n 3 michael     5\n 4 peter       5\n 5 marc        4\n 6 paul        4\n 7 richard     4\n 8 robert      4\n 9 scott       4\n10 george      3\n11 james       3\n12 joseph      3\n13 lee         3\n14 mike        3\n15 steven      3\n16 thomas      3\n\n\nWhile the top 5 directors are identical to the previous list, Marc appears in the 6th most frequent among directors of movies with an age difference greater than or equal to 15 years.\nLet’s find out which movies these Marc’s directed.\n\nage_gaps %>%\n   filter(grepl(\"Marc \", director)) %>%\n   arrange(desc(release_year))\n\n# A tibble: 14 × 15\n# Groups:   decade_released [2]\n   movie_name    relea…¹ direc…² age_d…³ coupl…⁴ actor…⁵ actor…⁶ chara…⁷ chara…⁸\n   <chr>           <dbl> <chr>     <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>  \n 1 The Only Liv…    2017 Marc W…      20       1 Pierce… Kate B… man     woman  \n 2 All I See Is…    2017 Marc F…      18       1 Jason … Blake … man     woman  \n 3 The Only Liv…    2017 Marc W…      17       2 Kate B… Callum… woman   man    \n 4 The Only Liv…    2017 Marc W…       3       3 Callum… Kierse… man     woman  \n 5 World War Z      2013 Marc F…      12       1 Brad P… Mireil… man     woman  \n 6 The Amazing …    2012 Marc W…       5       1 Andrew… Emma S… man     woman  \n 7 The Young Vi…    2009 Jean-M…       2       1 Rupert… Emily … man     woman  \n 8 500 Days of …    2009 Marc W…       1       1 Zooey … Joseph… woman   man    \n 9 Quantum of S…    2008 Marc F…      18       1 Daniel… Gemma … man     woman  \n10 Suburban Girl    2007 Marc K…      19       1 Alec B… Sarah … man     woman  \n11 Music and Ly…    2007 Marc L…      15       1 Hugh G… Drew B… man     woman  \n12 Stranger Tha…    2006 Marc F…      10       1 Will F… Maggie… man     woman  \n13 Two Weeks No…    2002 Marc L…       4       1 Hugh G… Sandra… man     woman  \n14 Monster's Ba…    2001 Marc F…      11       1 Billy … Halle … man     woman  \n# … with 6 more variables: actor_1_birthdate <date>, actor_2_birthdate <date>,\n#   actor_1_age <dbl>, actor_2_age <dbl>, decade_released <dbl>,\n#   max_age_diff <dbl>, and abbreviated variable names ¹​release_year,\n#   ²​director, ³​age_difference, ⁴​couple_number, ⁵​actor_1_name, ⁶​actor_2_name,\n#   ⁷​character_1_gender, ⁸​character_2_gender\n\n\nSo, it looks like our filter picked up a sneaky Marc… Jean-Marc. Let’s remove this observation by specifying that the director’s name must begin with Marc.\n\nmarc <- age_gaps %>%\n  filter(grepl(\"^Marc \", director))\n\nmarc %>%\n  pull(movie_name) %>%\n  unique()\n\n [1] \"The Only Living Boy in New York\" \"Suburban Girl\"                  \n [3] \"All I See Is You\"                \"Quantum of Solace\"              \n [5] \"Music and Lyrics\"                \"World War Z\"                    \n [7] \"Monster's Ball\"                  \"Stranger Than Fiction\"          \n [9] \"The Amazing Spider-Man\"          \"Two Weeks Notice\"               \n[11] \"500 Days of Summer\"             \n\n\nLet’s find the average age difference for these Marc-directed love movies\n\nmarc %>%\n  pull(age_difference) %>%\n  summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    5.00   12.00   11.77   18.00   20.00 \n\n\n\nmarc %>%\n  pull(director) %>%\n  unique()\n\n[1] \"Marc Webb\"     \"Marc Klein\"    \"Marc Forster\"  \"Marc Lawrence\"\n\n\nLet’s also rename the director variable and drop Marc from the rows using str_replace.\n\nlibrary(stringr)\n\nmarc$director <- str_replace(marc$director, \"Marc \", \"\")\n\nmarc <- marc %>% rename(`Director Marc` = director)\n\n\nmarc %>% ggplot(aes(x= release_year, y= age_difference, color= `Director Marc`)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = seq(2001, 2019, 3)) +\n  labs(x = \"Year of Movie Release\", y = \"Age Difference in Years\",\n       title = \"Age Gaps in Movies by Marc(s) by Year Released\")\n\n\n\n\nWhile my approach to this exercise was obviously silly, I learned a lot about the tidytext package, which I’d never used before, and a useful trick for a function I use regularly!\n\nwrite_csv(age_gaps, here(\"data/age_gaps_edited.csv\"))"
  },
  {
    "objectID": "visualization_exercise.html",
    "href": "visualization_exercise.html",
    "title": "Visualization Exercise",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nhere()\n\n[1] \"C:/Users/Hayley/Desktop/MADA2023/hayleyhemme-MADA-portfolio\"\n\n\nThe plot we’ll be trying to replicate is from Our World in Data.\n\n\n\n\n\n\nHIV_GBD <- read_csv(here(\"data/deaths-from-hiv-by-age.csv\"))\nglimpse(HIV_GBD)\n\nRows: 6,840\nColumns: 8\n$ Entity                                                      <chr> \"Afghanist…\n$ Code                                                        <chr> \"AFG\", \"AF…\n$ Year                                                        <dbl> 1990, 1991…\n$ `Deaths - HIV/AIDS - Sex: Both - Age: 70+ years (Number)`   <dbl> 1, 1, 2, 2…\n$ `Deaths - HIV/AIDS - Sex: Both - Age: 50-69 years (Number)` <dbl> 7, 8, 9, 1…\n$ `Deaths - HIV/AIDS - Sex: Both - Age: 15-49 years (Number)` <dbl> 15, 19, 24…\n$ `Deaths - HIV/AIDS - Sex: Both - Age: 5-14 years (Number)`  <dbl> 0, 0, 0, 1…\n$ `Deaths - HIV/AIDS - Sex: Both - Age: Under 5 (Number)`     <dbl> 10, 12, 13…\n\n\nLet’s group by year and find the sum of deaths for each year in each age group.\n\nyear_sums_70 <- HIV_GBD %>% group_by(Year) %>% \n  summarize(`70+ years` = sum(`Deaths - HIV/AIDS - Sex: Both - Age: 70+ years (Number)`))  \n\nyear_sums_50 <- HIV_GBD %>% group_by(Year) %>% \n  summarize(`50-69 years` = sum(`Deaths - HIV/AIDS - Sex: Both - Age: 50-69 years (Number)`))  \n\nyear_sums_15 <- HIV_GBD %>% group_by(Year) %>% summarize(`15-49 years`  = sum(`Deaths - HIV/AIDS - Sex: Both - Age: 15-49 years (Number)`))  \n\nyear_sums_5 <- HIV_GBD %>% group_by(Year) %>% \n  summarize(`5-14 years` = sum(`Deaths - HIV/AIDS - Sex: Both - Age: 5-14 years (Number)`)) \n\nyear_sums_0 <- HIV_GBD %>% group_by(Year) %>% \n  summarize(`Under 5 years` = sum(`Deaths - HIV/AIDS - Sex: Both - Age: Under 5 (Number)`)) \n\nLet’s join the datasets and check to see how things are looking.\n\nyear_sums <-list(year_sums_70, year_sums_50, year_sums_15, year_sums_5, year_sums_0)\nyear_sums <- year_sums %>% reduce(full_join)\n\nJoining with `by = join_by(Year)`\nJoining with `by = join_by(Year)`\nJoining with `by = join_by(Year)`\nJoining with `by = join_by(Year)`\n\nglimpse(year_sums)\n\nRows: 30\nColumns: 6\n$ Year            <dbl> 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, …\n$ `70+ years`     <dbl> 20399, 26573, 33797, 41998, 51362, 61170, 71366, 80974…\n$ `50-69 years`   <dbl> 206148, 265170, 334139, 414419, 505039, 596589, 676115…\n$ `15-49 years`   <dbl> 1169847, 1507786, 1901048, 2355302, 2857896, 3371592, …\n$ `5-14 years`    <dbl> 14380, 19461, 26079, 35140, 46940, 60284, 75037, 90767…\n$ `Under 5 years` <dbl> 371472, 459127, 553263, 651461, 747455, 839116, 919408…\n\n\nNice! Let’s make things a bit easier to plot by pivoting the data into long format.\n\nsums_long <- year_sums %>% \n  pivot_longer(2:6, names_to = \"Age\", values_to = \"Deaths\")\n\nWe’ll make age group a factor…\n\nsums_long <- sums_long %>%\n  mutate(Age = as.factor(Age), \n  Age = factor(\n      Age,\n      level = c(\"Under 5 years\", \"5-14 years\",\"15-49 years\", \"50-69 years\", \"70+ years\")))\n\nNow let’s plot it\n\nsums_long %>% ggplot(aes(x= Year, y = Deaths, color=Age, fill = Age))  +\n  geom_area() + labs(title = \"Deaths from HIV/AIDS , by age, World, 1990 to 2019\") + \n  theme_bw()\n\n\n\n\nWait! Something is not looking right with the data… our counts are significantly higher than those shown in the original plot. Let’s see if we find out why… Let’s load ‘naniar’ to see if there is anything unexpected about the data.\n\nlibrary(naniar)\nvis_miss(HIV_GBD)\n\n\n\n\nIt looks like the column ‘Entity’ contains data for both countries and continents! Let’s try to correct this by dropping observations missing a country code.\n\nHIV_GBD <- HIV_GBD %>% drop_na(Code)\n\n\nHIV_GBD %>% group_by(Year) %>%\n  slice_max(`Deaths - HIV/AIDS - Sex: Both - Age: 70+ years (Number)`)\n\n# A tibble: 30 × 8\n# Groups:   Year [30]\n   Entity Code      Year Deaths - HIV/AIDS - S…¹ Death…² Death…³ Death…⁴ Death…⁵\n   <chr>  <chr>    <dbl>                   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 World  OWID_WRL  1990                    3954   38533  217774    2712   73413\n 2 World  OWID_WRL  1991                    5152   49785  281693    3706   90819\n 3 World  OWID_WRL  1992                    6567   63068  356532    5013  109445\n 4 World  OWID_WRL  1993                    8175   78513  442911    6782  128770\n 5 World  OWID_WRL  1994                    9998   95881  538488    9084  147553\n 6 World  OWID_WRL  1995                   11907  113569  636910   11721  165293\n 7 World  OWID_WRL  1996                   13880  129592  723923   14650  180613\n 8 World  OWID_WRL  1997                   15741  144713  803647   17796  193410\n 9 World  OWID_WRL  1998                   17651  161922  898187   21356  205780\n10 World  OWID_WRL  1999                   19473  179766 1001519   25126  215227\n# … with 20 more rows, and abbreviated variable names\n#   ¹​`Deaths - HIV/AIDS - Sex: Both - Age: 70+ years (Number)`,\n#   ²​`Deaths - HIV/AIDS - Sex: Both - Age: 50-69 years (Number)`,\n#   ³​`Deaths - HIV/AIDS - Sex: Both - Age: 15-49 years (Number)`,\n#   ⁴​`Deaths - HIV/AIDS - Sex: Both - Age: 5-14 years (Number)`,\n#   ⁵​`Deaths - HIV/AIDS - Sex: Both - Age: Under 5 (Number)`\n\n\nThat explains it! Things were getting counted twice. Let’s making new dataframe where containing only observations for the ‘World’.\n\nworld <- HIV_GBD %>% \n  filter(grepl(\"OWID_WRL\", Code))\n\nRe-running the previous code…\n\nworld70 <- world %>% group_by(Year) %>% \n  summarize(`70+ years` = sum(`Deaths - HIV/AIDS - Sex: Both - Age: 70+ years (Number)`))  \n\nworld50 <- world %>% group_by(Year) %>% \n  summarize(`50-69 years` = sum(`Deaths - HIV/AIDS - Sex: Both - Age: 50-69 years (Number)`))  \n\nworld15 <- world %>% group_by(Year) %>% \n  summarize(`15-49 years`  = sum(`Deaths - HIV/AIDS - Sex: Both - Age: 15-49 years (Number)`))  \n\nworld5 <- world %>% group_by(Year) %>% \n  summarize(`5-14 years` = sum(`Deaths - HIV/AIDS - Sex: Both - Age: 5-14 years (Number)`)) \n\nworld0 <- world %>% group_by(Year) %>% \n  summarize(`Under 5 years` = sum(`Deaths - HIV/AIDS - Sex: Both - Age: Under 5 (Number)`)) \n\nworld <- list(world70, world50, world15, world5, world0)\nworld <- world %>% reduce(full_join)\n\nJoining with `by = join_by(Year)`\nJoining with `by = join_by(Year)`\nJoining with `by = join_by(Year)`\nJoining with `by = join_by(Year)`\n\nworld_long <- world %>% \n  pivot_longer(2:6, names_to = \"Age\", values_to = \"Deaths\") %>% \n  mutate(Age = as.factor(Age), \n  Age = factor(\n      Age,\n      level = c(\"Under 5 years\", \"5-14 years\",\"15-49 years\", \"50-69 years\", \"70+ years\")))\n\n\nworld_long %>% ggplot(aes(x= Year, y = Deaths, color=Age, fill = Age)) + \n  geom_area() + labs(title = \"Deaths from HIV/AIDS , by age, World, 1990 to 2019\")\n\n\n\n\nLet’s try to better match the original plot. We’ll first reverse the order of the age groups\n\nworld_long <- world_long %>% \n  mutate(Age = fct_rev(Age))\n\nThen we’ll load some useful packages. We’ll load ‘scales’ so that we can add ‘Million’ to the plot and RColorBrewer.\n\nlibrary(scales)\nlibrary(RColorBrewer)\nlibrary(extrafont)\n\n\nplot <- world_long %>% ggplot(aes(x= Year, y = Deaths, color = Age, fill = Age)) +\n  geom_area(alpha = 0.7) + \n  labs(title = \"Deaths from HIV/AIDS , by age, World, 1990 to 2019\") + \n  theme_bw() + scale_fill_brewer(palette = \"Oranges\",\ndirection = -1) + scale_color_brewer(palette = \"Oranges\", direction = -1) +\n  theme(plot.title = element_text(family = \"serif\")) +\n  theme(axis.title.x=element_blank(), axis.title.y=\n          element_blank(), axis.ticks.y = element_blank()) +\n  scale_x_continuous(breaks=c(1990, 1995, 2000, 2005, 2010, 2015, 2019)) + \n  scale_y_continuous(breaks=c(2e5, 4e5, 6e5, 8e5, 1e6, 1.2e6, 1.4e6, 1.6e6, 1.8e6), \n labels = c(\"200,000\",\"400,000\",\"600,000\",\"800,000\" , \"1 Million\", \"1.2 Million\", \n            \"1.4 Million\", \"1.6 Million\", \"1.8 Million\")) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_line(\n    linetype = \"dashed\"),\npanel.border = element_blank(), axis.line.x = element_line(color = \"gray\"))\n\nplot\n\n\n\n\nNot exactly perfect, but we’re getting pretty close!\nAbout the code– alpha is used to the change transparency; I reversed the order of colors in the scale_x_brewer by using direction = -1; I manually specified my breaks for both axes, and add a label to the y axis. I removed major grid-lines and the border around the plot using theme and element blank, and changed the line type of the minor grid-lines, made the x axis line gray.\nSome websites I referred to were:\nggplot2 Reference and Examples (Part 2) - Colours\nStatistics Globe\nLet’s try to directly label the age groups to the plot to better match the original. First, we’ll need to subset the data to just the last observation for x.\n\nw_19 <- world_long %>% filter(Year == 2019)\n\nLet’s also make a new vector containing the age groups.\n\nag <- factor(c(\"Under 5 years\", \"5-14 years\",\"15-49 years\", \"50-69 years\", \"70+ years\"))\nag<- factor(ag, level = c(\"Under 5 years\", \"5-14 years\",\"15-49 years\", \"50-69 years\", \"70+ years\"), fct_rev(ag))\n\nLet’s see how this works…\n\nplot + geom_text(data = w_19, aes(x = 2021.5, y = c(8.7e5, 7.5e5, 4e5, 8.5e4, 100), label = Age),\n    alpha= 2) + theme(legend.position = \"none\") +\n      scale_y_continuous(breaks=c(2e5, 4e5, 6e5, 8e5, 1e6, 1.2e6, 1.4e6, 1.6e6, 1.8e6), \n labels = c(\"200,000\",\"400,000\",\"600,000\",\"800,000\" , \"1 Million\", \"1.2 Million\", \n            \"1.4 Million\", \"1.6 Million\", \"1.8 Million\"),\n sec.axis = sec_axis(~ ., breaks = c(8.7e5, 7.5e5, 4e5, 8.5e4, 100, labels = ag)))\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\nNot how I hoped, but I think that I’m on the right track… Some things that I definitely want to work on are making the ‘Under 5 Years’ label more visible and adding direct labels onto the plot.\nSomething to note about this is that when I called scale_y_continuous, it overwrote the previous y scale with labels that I had specified. To get around this, I added that part again and it worked well. Notice the warning message."
  },
  {
    "objectID": "dataanalysis_exercise.html",
    "href": "dataanalysis_exercise.html",
    "title": "Data Analysis Exercise",
    "section": "",
    "text": "This dataset describes drug poisoning deaths in the United States from 1999 - 2015 by demographic factors, including age group, sex, and a simple race/ethnicity variable (NH White, NH Black, Hispanic). This dataset includes all drug poisoning deaths– intentional(suicide and homicide), unintentional, and those with undetermined intent. While this dataset includes some data at the state-level, further investigation into trends of drug poisoning deaths at the state level is limited due to the fact that the data have been aggregated.\nLet’s load the packages we’ll be using in this analysis.\nLoading the dataset…\nLet’s take a look at the data.\nLet’s remove observations at the state-level by filtering for only observations where ‘state’ is the “United States” and remove columns that we won’t be using in the analysis.\nNext, let’s reclass some of our variables. We will also reorder the dataset so that Year is sorted chronologically, then by age group and sex.\nNext, let’s remove observations where rates have been aggregated and remove the columns with NA’s,\nTo make some of these variable easier to work with, let’s rename them.\nLet’s see if we can extract some data from other minorities….\nBefore starting data visualization, let’s reorder the race/ethnicity variable by decreasing crude death rate so that our legend for our plots is easier to interpret.\nLet’s clear the environment and start fresh by loading up the RDS file we just saved!\nSummary Table\nData Visualization and next steps…\nMy idea with this dataset is to explore trends in drug poisoning deaths across a number of demographic factors. Variables of interest are age group (especially ages 25-44), sex, and race/ethnicity."
  },
  {
    "objectID": "coding_exercise.html#continued-data-visualization-and-analysis",
    "href": "coding_exercise.html#continued-data-visualization-and-analysis",
    "title": "R Coding Exercise",
    "section": "Continued Data Visualization and Analysis",
    "text": "Continued Data Visualization and Analysis\n\nCountry Contributions to Continental GDP\n\n# How Much Does Each Country Contribute to Continent's Overall GDP?\n## Setting Up Initial Data\nregion_and_gdp <- africadata %>% filter(year == 1983) %>% group_by(region, gdp) %>% \n  select(country, region, gdp)\n## Creating a New Variable for Continent's GDP\nregion_and_gdp$total_gdp <- sum(region_and_gdp$gdp, na.rm = TRUE)\n# Percentage Contributed to Each \nregion_and_gdp <- region_and_gdp %>%\n  mutate(percentage = gdp/total_gdp * 100)\n\n# Graphing African Countries' Contributions to Continent's GDP\nggplot(region_and_gdp, aes(x = percentage, y = country, group = region, fill = region)) + geom_col() + labs(x = \"Percentage Contributed to Continent's GDP\", y = \"Nation\", title = \"Each Nation's Contribution to Continent's GDP in 1983\")\n\nWarning: Removed 7 rows containing missing values (`position_stack()`).\n\n\n\n\n\n\n\nLooking at What Factors Affect Population in Asia\n\nlibrary(broom)\n\nWarning: package 'broom' was built under R version 4.2.2\n\n# Creating a Model\npopulation_factors_model <- lm(population ~ fertility + life_expectancy + infant_mortality, data = gapminder %>% filter(continent == \"Asia\"))\n\n# Evaluating Model\npop_factors_model_table <- tidy(summary(population_factors_model))\n\nAccording to this model, fertility (p < 0.001) and life expectancy (p < 0.001) appear to significantly impact population in Asia since the null hypothesis that they do not is rejected. However, the null hypothesis that infant mortality does not affect population, in contrast, isn’t rejected."
  },
  {
    "objectID": "coding_exercise.html#fertility-over-time-in-europe",
    "href": "coding_exercise.html#fertility-over-time-in-europe",
    "title": "R Coding Exercise",
    "section": "Fertility Over Time in Europe",
    "text": "Fertility Over Time in Europe\n\n# Investigating Fertility Across Time\nggplot(gapminder %>% filter(continent == \"Europe\"), aes(x = year, y = fertility, group = region, color = region)) + geom_point() + labs(x = \"Year\", y = \"Fertility Rate\", title = \"Fertility Over Time in Europe\")\n\nWarning: Removed 39 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "coding_exercise.html#tracking-australias-gdp-over-time",
    "href": "coding_exercise.html#tracking-australias-gdp-over-time",
    "title": "R Coding Exercise",
    "section": "Tracking Australia’s GDP Over Time",
    "text": "Tracking Australia’s GDP Over Time\n\n# Investigating Australia's GDP Across Time\nggplot(gapminder %>% filter(country == \"Australia\"), aes(x = year, y = gdp, color = year)) + geom_point() + geom_line() + labs(x = \"Year\", y = \"GDP\", title = \"GDP Over Time for Australia\")\n\nWarning: Removed 5 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 5 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "dataanalysis_exercise.html#this-section-added-by-connor-ross",
    "href": "dataanalysis_exercise.html#this-section-added-by-connor-ross",
    "title": "Data Analysis Exercise",
    "section": "This section added by CONNOR ROSS",
    "text": "This section added by CONNOR ROSS\nHi Hayley! Thanks for the Intro. This is a really meaningful and cool topic.\n\n# You know I got to have some tidyverseeeee...\nlibrary(tidyverse)\n\n\n# Let's take a look at what we got...\nstr(df)\n\ntibble [1,088 × 7] (S3: tbl_df/tbl/data.frame)\n $ Year      : num [1:1088] 1999 1999 1999 1999 1999 ...\n $ Sex       : Factor w/ 3 levels \"Both Sexes\",\"Female\",..: 2 3 2 3 2 3 2 3 2 3 ...\n $ Age       : Factor w/ 9 levels \"Less than 15 years\",..: 1 1 2 2 3 3 4 4 5 5 ...\n $ race_eth  : Factor w/ 4 levels \"Non-Hispanic White\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ Deaths    : num [1:1088] 11 11 26 78 126 302 305 631 177 551 ...\n $ Population: num [1:1088] 4604949 4748293 2799428 2761491 2752017 ...\n $ crude_dr  : num [1:1088] 0.2 0.2 0.9 2.8 4.6 12.2 10.4 24.4 8.3 30.4 ...\n\nsummary(df)\n\n      Year              Sex                      Age     \n Min.   :1999   Both Sexes:  0   Less than 15 years:136  \n 1st Qu.:2003   Female    :544   15-24 years       :136  \n Median :2007   Male      :544   25-34 years       :136  \n Mean   :2007                    35-44 years       :136  \n 3rd Qu.:2011                    45-54 years       :136  \n Max.   :2015                    55-64 years       :136  \n                                 (Other)           :272  \n                    race_eth       Deaths          Population      \n Non-Hispanic White     :272   Min.   :   0.00   Min.   :  146555  \n Non-Hispanic Black     :272   1st Qu.:  21.75   1st Qu.: 1161401  \n Hispanic               :272   Median : 102.50   Median : 2585722  \n Non-Hispanic Other Race:272   Mean   : 522.58   Mean   : 4699098  \n                               3rd Qu.: 388.25   3rd Qu.: 6651233  \n                               Max.   :6874.00   Max.   :19270575  \n                                                                   \n    crude_dr     \n Min.   : 0.000  \n 1st Qu.: 2.300  \n Median : 4.600  \n Mean   : 8.045  \n 3rd Qu.:11.300  \n Max.   :53.500  \n                 \n\n# Sweet summary stats but I'm more of a visual person...\n\n## Aggregate Death Count by Sex\ndf %>%\n  ggplot(aes(x = factor(Sex), y = Deaths, fill = factor(Sex))) + \n  geom_bar(stat = \"identity\") +\n  labs(title = \"Deaths by Sex\", x = \"Sex (Male or Female)\",\n       y = \"Aggregate Deaths (All Age Groups from 1999 - 2015)\", fill = \"Sex\") +\n  theme_dark()\n\n\n\n## Aggregate Death Count by Race/Ethnicity\ndf %>%\n  ggplot(aes(x = factor(race_eth), y = Deaths, fill = factor(race_eth))) + \n  geom_bar(stat = \"identity\") +\n  labs(title = \"Drug Poison Deaths by Race/Ethnicity\", x = \"Race/Ethnicity\",\n       y = \"Aggregate Deaths (All Races/Ethnicities from 1999 - 2015)\",\n       fill = \"Race/Ethnicity\") +\n  theme_dark()\n\n\n\ndf %>%\n  ggplot(aes(x = factor(race_eth), y = log(Deaths), fill = factor(race_eth))) + \n  geom_bar(stat = \"identity\") +\n  labs(title = \"Log Drug Poison Deaths by Race/Ethnicity in the US\", \n       x = \"Race/Ethnicity\",\n       y = \"Log Deaths (All Races/Ethnicities from 1999 - 2015)\",\n       fill = \"Race/Ethnicity\") +\n  theme_dark()\n\nWarning: Removed 7 rows containing missing values (`geom_bar()`).\n\n\n\n\n## Aggregate Death Count by Age Group\ndf %>%\n  ggplot(aes(x = factor(Age), y = Deaths, fill = factor(Age))) + \n  geom_bar(stat = \"identity\") +\n  labs(title = \"Deaths by Age\", x = \"Age Group\",\n       y = \"Aggregate Deaths (from 1999 - 2015)\", fill = \"Age\") +\n  theme_dark()\n\n\n\ndf %>%\n  filter(Age == \"25-34 years\" | Age == \"35-44 years\") %>%\n  ggplot(aes(x = factor(Age), y = Deaths, fill = factor(Age))) + \n  geom_bar(stat = \"identity\") +\n  labs(title = \"Deaths by Age\", x = \"Age Group\",\n       y = \"Aggregate Deaths (from 1999 - 2015)\", fill = \"Age\") +\n  theme_dark()\n\n\n\n## CDR Trends by Age Group\ndf %>%\n  ggplot(aes(x = Year, y = crude_dr, color = Age)) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Drug Poison Death Rate Trends by Age Group in the US\", \n       x = \"Year of Death\", \n       y = \"Crude Death Rate (per 100,000)\") +\n  theme_dark()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n### CDR Trends by Sex\ndf %>%\n  ggplot(aes(x = Year, y = crude_dr, color = Sex)) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Drug Poison Death Rate Trends by Sex in the US\", \n       x = \"Year of Death\", \n       y = \"Crude Death Rate (per 100,000)\") +\n  theme_dark()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\ndf %>%\n  filter(Age == \"25-34 years\" | Age == \"35-44 years\") %>%\n  ggplot(aes(x = Year, y = crude_dr, color = Age)) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Drug Poison Death Rate Trends by Age Group in the US\", \n       x = \"Year of Death\", \n       y = \"Crude Death Rate (per 100,000)\") +\n  theme_dark()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "fluanalysis/code/exploration.html",
    "href": "fluanalysis/code/exploration.html",
    "title": "Exploration",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\n\n\nflu <- readRDS(here(\"fluanalysis/data/processed_data/flu.rds\"))\n\nLet’s take a look at some summary statistics for BodyTemp and Nausea\n\nsummary(flu$BodyTemp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  97.20   98.20   98.50   98.94   99.30  103.10 \n\nsummary(flu$Nausea)\n\n No Yes \n475 255 \n\n\nNext, let’s take a look at this distribution of BodyTemp.\n\nflu  %>% ggplot(aes(x = BodyTemp))  + geom_histogram(bins = 20)\n\n\n\n\nI’m also interested in the intensity variables… Let’s make a box plot of a couple of these with BodyTemp.\n\nflu %>% ggplot(aes(x= BodyTemp, y = Myalgia, color = Myalgia)) +\n  geom_boxplot() \n\n\n\nflu %>% ggplot(aes(x= BodyTemp, y = Weakness, color = Weakness)) +\n  geom_boxplot()\n\n\n\n\nMedian body temperature appears to increase with increasing intensity of myalgia/ weakness.\nLet’s look at this as a histogram for weakness.\n\nflu  %>% ggplot(aes(x = BodyTemp, fill = Weakness)) + \n  geom_histogram(bins = 20) \n\n\n\n\nWeakness by Nausea contingency table\n\ntable(flu$Weakness,flu$Nausea)\n\n          \n            No Yes\n  None      39  10\n  Mild     172  51\n  Moderate 210 128\n  Severe    54  66\n\n\nMyalgia by Nausea contingency table\n\ntable(flu$Myalgia,flu$Nausea)\n\n          \n            No Yes\n  None      63  16\n  Mild     159  54\n  Moderate 198 127\n  Severe    55  58\n\n\nCough Intensity by Nausea contingency table\n\ntable(flu$CoughIntensity,flu$Nausea)\n\n          \n            No Yes\n  None      30  17\n  Mild      99  55\n  Moderate 232 125\n  Severe   114  58\n\n\nNow let’s take visualize this.\n\nflu %>% ggplot(aes(x= Weakness, fill = CoughIntensity)) +\n  geom_histogram(stat=\"count\")\n\n\n\nflu %>% ggplot(aes(x= Weakness, fill = Myalgia)) +\n  geom_histogram(stat=\"count\")\n\n\n\nflu %>% ggplot(aes(x= Weakness, fill = Nausea)) +\n  geom_histogram(stat=\"count\")\n\n\n\n\nWeakness by Nausea contingency table\n\ntable(flu$Weakness,flu$Myalgia)\n\n          \n           None Mild Moderate Severe\n  None       22   22        5      0\n  Mild       37  120       62      4\n  Moderate   18   64      208     48\n  Severe      2    7       50     61"
  },
  {
    "objectID": "fluanalysis/code/wrangling.html",
    "href": "fluanalysis/code/wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "Load packages\n\nlibrary(here)\nlibrary(tidyverse)\n\nLoad the data\n\nflu <- readRDS(here(\"fluanalysis/data/raw_data/SympAct_Any_Pos.Rda\"))\n\nLet’s take a look at the dataset using glimpse\n\nglimpse(flu)\n\nRows: 735\nColumns: 63\n$ DxName1           <fct> \"Influenza like illness - Clinical Dx\", \"Acute tonsi…\n$ DxName2           <fct> NA, \"Influenza like illness - Clinical Dx\", \"Acute p…\n$ DxName3           <fct> NA, NA, NA, NA, NA, NA, NA, NA, \"Fever, unspecified\"…\n$ DxName4           <fct> NA, NA, NA, NA, NA, NA, NA, NA, \"Other fatigue\", NA,…\n$ DxName5           <fct> NA, NA, NA, NA, NA, NA, NA, NA, \"Headache\", NA, NA, …\n$ Unique.Visit      <chr> \"340_17632125\", \"340_17794836\", \"342_17737773\", \"342…\n$ ActivityLevel     <int> 10, 6, 2, 2, 5, 3, 4, 0, 0, 5, 9, 1, 3, 6, 5, 2, 2, …\n$ ActivityLevelF    <fct> 10, 6, 2, 2, 5, 3, 4, 0, 0, 5, 9, 1, 3, 6, 5, 2, 2, …\n$ SwollenLymphNodes <fct> Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y…\n$ ChestCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ ChillsSweats      <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, …\n$ NasalCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ CoughYN           <fct> Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, …\n$ Sneeze            <fct> No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, …\n$ Fatigue           <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ SubjectiveFever   <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes…\n$ Headache          <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes…\n$ Weakness          <fct> Mild, Severe, Severe, Severe, Moderate, Moderate, Mi…\n$ WeaknessYN        <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ CoughIntensity    <fct> Severe, Severe, Mild, Moderate, None, Moderate, Seve…\n$ CoughYN2          <fct> Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes…\n$ Myalgia           <fct> Mild, Severe, Severe, Severe, Mild, Moderate, Mild, …\n$ MyalgiaYN         <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ RunnyNose         <fct> No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No…\n$ AbPain            <fct> No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N…\n$ ChestPain         <fct> No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, …\n$ Diarrhea          <fct> No, No, No, No, No, Yes, No, No, No, No, No, No, No,…\n$ EyePn             <fct> No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye…\n$ Insomnia          <fct> No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y…\n$ ItchyEye          <fct> No, No, No, No, No, No, No, No, No, No, No, No, Yes,…\n$ Nausea            <fct> No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y…\n$ EarPn             <fct> No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y…\n$ Hearing           <fct> No, Yes, No, No, No, No, No, No, No, No, No, No, No,…\n$ Pharyngitis       <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, …\n$ Breathless        <fct> No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, …\n$ ToothPn           <fct> No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N…\n$ Vision            <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ Vomit             <fct> No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N…\n$ Wheeze            <fct> No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N…\n$ BodyTemp          <dbl> 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, …\n$ RapidFluA         <fct> Presumptive Negative For Influenza A, NA, Presumptiv…\n$ RapidFluB         <fct> Presumptive Negative For Influenza B, NA, Presumptiv…\n$ PCRFluA           <fct> NA, NA, NA, NA, NA, NA,  Influenza A Not Detected, N…\n$ PCRFluB           <fct> NA, NA, NA, NA, NA, NA,  Influenza B Not Detected, N…\n$ TransScore1       <dbl> 1, 3, 4, 5, 0, 2, 2, 5, 4, 4, 2, 3, 2, 5, 3, 5, 1, 5…\n$ TransScore1F      <fct> 1, 3, 4, 5, 0, 2, 2, 5, 4, 4, 2, 3, 2, 5, 3, 5, 1, 5…\n$ TransScore2       <dbl> 1, 2, 3, 4, 0, 2, 2, 4, 3, 3, 1, 2, 2, 4, 2, 4, 1, 4…\n$ TransScore2F      <fct> 1, 2, 3, 4, 0, 2, 2, 4, 3, 3, 1, 2, 2, 4, 2, 4, 1, 4…\n$ TransScore3       <dbl> 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 0, 1, 1, 3, 1, 3, 1, 3…\n$ TransScore3F      <fct> 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 0, 1, 1, 3, 1, 3, 1, 3…\n$ TransScore4       <dbl> 0, 2, 4, 4, 0, 1, 1, 4, 3, 3, 2, 2, 2, 4, 3, 4, 0, 4…\n$ TransScore4F      <fct> 0, 2, 4, 4, 0, 1, 1, 4, 3, 3, 2, 2, 2, 4, 3, 4, 0, 4…\n$ ImpactScore       <int> 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9…\n$ ImpactScore2      <int> 6, 7, 13, 11, 10, 11, 7, 6, 9, 6, 12, 16, 10, 12, 8,…\n$ ImpactScore3      <int> 3, 4, 9, 7, 6, 7, 3, 3, 6, 4, 7, 11, 6, 8, 4, 4, 5, …\n$ ImpactScoreF      <fct> 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9…\n$ ImpactScore2F     <fct> 6, 7, 13, 11, 10, 11, 7, 6, 9, 6, 12, 16, 10, 12, 8,…\n$ ImpactScore3F     <fct> 3, 4, 9, 7, 6, 7, 3, 3, 6, 4, 7, 11, 6, 8, 4, 4, 5, …\n$ ImpactScoreFD     <fct> 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9…\n$ TotalSymp1        <dbl> 8, 11, 18, 17, 11, 14, 10, 12, 14, 11, 15, 20, 13, 1…\n$ TotalSymp1F       <fct> 8, 11, 18, 17, 11, 14, 10, 12, 14, 11, 15, 20, 13, 1…\n$ TotalSymp2        <dbl> 8, 10, 17, 16, 11, 14, 10, 11, 13, 10, 14, 19, 13, 1…\n$ TotalSymp3        <dbl> 8, 9, 16, 15, 11, 14, 10, 10, 12, 9, 13, 18, 12, 16,…\n\n\nNow that we have an idea of what the data looks like, let’s drop variables we won’t be using in our analysis using select and remove missing observations.\n\nflu <- flu %>%\n  select(-contains(c(\"Score\", \"Flu\")),\n         -starts_with(c(\"DxName\", \"Activity\", \"Total\", \"Unique.Visit\"))) %>%\n  drop_na()\n\nglimpse(flu)\n\nRows: 730\nColumns: 32\n$ SwollenLymphNodes <fct> Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y…\n$ ChestCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ ChillsSweats      <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, …\n$ NasalCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ CoughYN           <fct> Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, …\n$ Sneeze            <fct> No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, …\n$ Fatigue           <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ SubjectiveFever   <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes…\n$ Headache          <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes…\n$ Weakness          <fct> Mild, Severe, Severe, Severe, Moderate, Moderate, Mi…\n$ WeaknessYN        <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ CoughIntensity    <fct> Severe, Severe, Mild, Moderate, None, Moderate, Seve…\n$ CoughYN2          <fct> Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes…\n$ Myalgia           <fct> Mild, Severe, Severe, Severe, Mild, Moderate, Mild, …\n$ MyalgiaYN         <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ RunnyNose         <fct> No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No…\n$ AbPain            <fct> No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N…\n$ ChestPain         <fct> No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, …\n$ Diarrhea          <fct> No, No, No, No, No, Yes, No, No, No, No, No, No, No,…\n$ EyePn             <fct> No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye…\n$ Insomnia          <fct> No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y…\n$ ItchyEye          <fct> No, No, No, No, No, No, No, No, No, No, No, No, Yes,…\n$ Nausea            <fct> No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y…\n$ EarPn             <fct> No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y…\n$ Hearing           <fct> No, Yes, No, No, No, No, No, No, No, No, No, No, No,…\n$ Pharyngitis       <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, …\n$ Breathless        <fct> No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, …\n$ ToothPn           <fct> No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N…\n$ Vision            <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ Vomit             <fct> No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N…\n$ Wheeze            <fct> No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N…\n$ BodyTemp          <dbl> 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, …\n\n\n\nsaveRDS(flu, here(\"fluanalysis/data/processed_data/flu.rds\"))"
  },
  {
    "objectID": "wrangling.html",
    "href": "wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "Load packages\n\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.2.2\n\n\nhere() starts at C:/Users/Hayley/Desktop/MADA2023/hayleyhemme-MADA-portfolio\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.2\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\n\nWarning: package 'readr' was built under R version 4.2.2\n\n\nWarning: package 'purrr' was built under R version 4.2.2\n\n\nWarning: package 'dplyr' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\nWarning: package 'forcats' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nLoad the data\n\nflu <- readRDS(here(\"fluanalysis/data/raw_data/SympAct_Any_Pos.Rda\"))\n\nLet’s take a look at the dataset using glimpse\n\nglimpse(flu)\n\nRows: 735\nColumns: 63\n$ DxName1           <fct> \"Influenza like illness - Clinical Dx\", \"Acute tonsi…\n$ DxName2           <fct> NA, \"Influenza like illness - Clinical Dx\", \"Acute p…\n$ DxName3           <fct> NA, NA, NA, NA, NA, NA, NA, NA, \"Fever, unspecified\"…\n$ DxName4           <fct> NA, NA, NA, NA, NA, NA, NA, NA, \"Other fatigue\", NA,…\n$ DxName5           <fct> NA, NA, NA, NA, NA, NA, NA, NA, \"Headache\", NA, NA, …\n$ Unique.Visit      <chr> \"340_17632125\", \"340_17794836\", \"342_17737773\", \"342…\n$ ActivityLevel     <int> 10, 6, 2, 2, 5, 3, 4, 0, 0, 5, 9, 1, 3, 6, 5, 2, 2, …\n$ ActivityLevelF    <fct> 10, 6, 2, 2, 5, 3, 4, 0, 0, 5, 9, 1, 3, 6, 5, 2, 2, …\n$ SwollenLymphNodes <fct> Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y…\n$ ChestCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ ChillsSweats      <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, …\n$ NasalCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ CoughYN           <fct> Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, …\n$ Sneeze            <fct> No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, …\n$ Fatigue           <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ SubjectiveFever   <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes…\n$ Headache          <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes…\n$ Weakness          <fct> Mild, Severe, Severe, Severe, Moderate, Moderate, Mi…\n$ WeaknessYN        <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ CoughIntensity    <fct> Severe, Severe, Mild, Moderate, None, Moderate, Seve…\n$ CoughYN2          <fct> Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes…\n$ Myalgia           <fct> Mild, Severe, Severe, Severe, Mild, Moderate, Mild, …\n$ MyalgiaYN         <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ RunnyNose         <fct> No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No…\n$ AbPain            <fct> No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N…\n$ ChestPain         <fct> No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, …\n$ Diarrhea          <fct> No, No, No, No, No, Yes, No, No, No, No, No, No, No,…\n$ EyePn             <fct> No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye…\n$ Insomnia          <fct> No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y…\n$ ItchyEye          <fct> No, No, No, No, No, No, No, No, No, No, No, No, Yes,…\n$ Nausea            <fct> No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y…\n$ EarPn             <fct> No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y…\n$ Hearing           <fct> No, Yes, No, No, No, No, No, No, No, No, No, No, No,…\n$ Pharyngitis       <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, …\n$ Breathless        <fct> No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, …\n$ ToothPn           <fct> No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N…\n$ Vision            <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ Vomit             <fct> No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N…\n$ Wheeze            <fct> No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N…\n$ BodyTemp          <dbl> 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, …\n$ RapidFluA         <fct> Presumptive Negative For Influenza A, NA, Presumptiv…\n$ RapidFluB         <fct> Presumptive Negative For Influenza B, NA, Presumptiv…\n$ PCRFluA           <fct> NA, NA, NA, NA, NA, NA,  Influenza A Not Detected, N…\n$ PCRFluB           <fct> NA, NA, NA, NA, NA, NA,  Influenza B Not Detected, N…\n$ TransScore1       <dbl> 1, 3, 4, 5, 0, 2, 2, 5, 4, 4, 2, 3, 2, 5, 3, 5, 1, 5…\n$ TransScore1F      <fct> 1, 3, 4, 5, 0, 2, 2, 5, 4, 4, 2, 3, 2, 5, 3, 5, 1, 5…\n$ TransScore2       <dbl> 1, 2, 3, 4, 0, 2, 2, 4, 3, 3, 1, 2, 2, 4, 2, 4, 1, 4…\n$ TransScore2F      <fct> 1, 2, 3, 4, 0, 2, 2, 4, 3, 3, 1, 2, 2, 4, 2, 4, 1, 4…\n$ TransScore3       <dbl> 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 0, 1, 1, 3, 1, 3, 1, 3…\n$ TransScore3F      <fct> 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 0, 1, 1, 3, 1, 3, 1, 3…\n$ TransScore4       <dbl> 0, 2, 4, 4, 0, 1, 1, 4, 3, 3, 2, 2, 2, 4, 3, 4, 0, 4…\n$ TransScore4F      <fct> 0, 2, 4, 4, 0, 1, 1, 4, 3, 3, 2, 2, 2, 4, 3, 4, 0, 4…\n$ ImpactScore       <int> 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9…\n$ ImpactScore2      <int> 6, 7, 13, 11, 10, 11, 7, 6, 9, 6, 12, 16, 10, 12, 8,…\n$ ImpactScore3      <int> 3, 4, 9, 7, 6, 7, 3, 3, 6, 4, 7, 11, 6, 8, 4, 4, 5, …\n$ ImpactScoreF      <fct> 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9…\n$ ImpactScore2F     <fct> 6, 7, 13, 11, 10, 11, 7, 6, 9, 6, 12, 16, 10, 12, 8,…\n$ ImpactScore3F     <fct> 3, 4, 9, 7, 6, 7, 3, 3, 6, 4, 7, 11, 6, 8, 4, 4, 5, …\n$ ImpactScoreFD     <fct> 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9…\n$ TotalSymp1        <dbl> 8, 11, 18, 17, 11, 14, 10, 12, 14, 11, 15, 20, 13, 1…\n$ TotalSymp1F       <fct> 8, 11, 18, 17, 11, 14, 10, 12, 14, 11, 15, 20, 13, 1…\n$ TotalSymp2        <dbl> 8, 10, 17, 16, 11, 14, 10, 11, 13, 10, 14, 19, 13, 1…\n$ TotalSymp3        <dbl> 8, 9, 16, 15, 11, 14, 10, 10, 12, 9, 13, 18, 12, 16,…\n\n\nNow that we have an idea of what the data looks like, let’s drop variables we won’t be using in our analysis using select and remove missing observations.\n\nflu <- flu %>%\n  select(-contains(c(\"Score\", \"Flu\")),\n         -starts_with(c(\"DxName\", \"Activity\", \"Total\", \"Unique.Visit\"))) %>%\n  drop_na()\n\nglimpse(flu)\n\nRows: 730\nColumns: 32\n$ SwollenLymphNodes <fct> Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y…\n$ ChestCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ ChillsSweats      <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, …\n$ NasalCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ CoughYN           <fct> Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, …\n$ Sneeze            <fct> No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, …\n$ Fatigue           <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ SubjectiveFever   <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes…\n$ Headache          <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes…\n$ Weakness          <fct> Mild, Severe, Severe, Severe, Moderate, Moderate, Mi…\n$ WeaknessYN        <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ CoughIntensity    <fct> Severe, Severe, Mild, Moderate, None, Moderate, Seve…\n$ CoughYN2          <fct> Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes…\n$ Myalgia           <fct> Mild, Severe, Severe, Severe, Mild, Moderate, Mild, …\n$ MyalgiaYN         <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ RunnyNose         <fct> No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No…\n$ AbPain            <fct> No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N…\n$ ChestPain         <fct> No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, …\n$ Diarrhea          <fct> No, No, No, No, No, Yes, No, No, No, No, No, No, No,…\n$ EyePn             <fct> No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye…\n$ Insomnia          <fct> No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y…\n$ ItchyEye          <fct> No, No, No, No, No, No, No, No, No, No, No, No, Yes,…\n$ Nausea            <fct> No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y…\n$ EarPn             <fct> No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y…\n$ Hearing           <fct> No, Yes, No, No, No, No, No, No, No, No, No, No, No,…\n$ Pharyngitis       <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, …\n$ Breathless        <fct> No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, …\n$ ToothPn           <fct> No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N…\n$ Vision            <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ Vomit             <fct> No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N…\n$ Wheeze            <fct> No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N…\n$ BodyTemp          <dbl> 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, …\n\n\n\nsaveRDS(flu, here(\"fluanalysis/data/processed_data/flu.rds\"))"
  },
  {
    "objectID": "fluanalysis/code/fitting.html",
    "href": "fluanalysis/code/fitting.html",
    "title": "Model fitting",
    "section": "",
    "text": "library(here)\nlibrary(tidymodels)\nlibrary(performance)\nlibrary(tidyverse)\n\n\nflu <- readRDS(here(\"fluanalysis/data/processed_data/flu.rds\"))\n\nUnivariate model for\n\nlm_mod <- linear_reg() %>% \n  set_engine(\"lm\") %>% \n  fit(BodyTemp ~ Weakness, data = flu)\n\ntidy(lm_mod)\n\n# A tibble: 4 × 5\n  term             estimate std.error statistic p.value\n  <chr>               <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        98.6       0.170    580.   0      \n2 WeaknessMild        0.256     0.188      1.37 0.172  \n3 WeaknessModerate    0.317     0.182      1.74 0.0816 \n4 WeaknessSevere      0.619     0.202      3.07 0.00221\n\nglance(lm_mod)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1  0.0159  0.0119  1.19    3.92 0.00857     3 -1160. 2331. 2354.   1027.     726\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\nMultivariate model (full)\n\nlm_full <- linear_reg() %>%\n  set_engine(\"lm\") %>% \n  fit(BodyTemp ~ . ,data = flu)\n\ntidy(lm_full)\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic   p.value\n   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)           97.9       0.304   322.     0        \n 2 SwollenLymphNodesYes  -0.165     0.0920   -1.80   0.0727   \n 3 ChestCongestionYes     0.0873    0.0975    0.895  0.371    \n 4 ChillsSweatsYes        0.201     0.127     1.58   0.114    \n 5 NasalCongestionYes    -0.216     0.114    -1.90   0.0584   \n 6 CoughYNYes             0.314     0.241     1.30   0.193    \n 7 SneezeYes             -0.362     0.0983   -3.68   0.000249 \n 8 FatigueYes             0.265     0.161     1.65   0.0996   \n 9 SubjectiveFeverYes     0.437     0.103     4.22   0.0000271\n10 HeadacheYes            0.0115    0.125     0.0913 0.927    \n# … with 28 more rows\n\nglance(lm_full)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1   0.129  0.0860  1.14    3.02 4.20e-8    34 -1116. 2304. 2469.    909.     695\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\nComparing model performance for univariate vs multivariate linear model.\n\ncompare_performance(lm_mod, lm_full)\n\n# Comparison of Model Performance Indices\n\nName    | Model |  AIC (weights) | AICc (weights) |  BIC (weights) |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------------------------------------------\nlm_mod  |   _lm | 2330.7 (<.001) | 2330.7 (<.001) | 2353.6 (>.999) | 0.016 |     0.012 | 1.186 | 1.189\nlm_full |   _lm | 2303.8 (>.999) | 2307.7 (>.999) | 2469.2 (<.001) | 0.129 |     0.086 | 1.116 | 1.144\n\n\nThe full model appears to be a better fit than univariate model with a lower AIC of 2307.8 and higher R2(adj.) of 0.086.\nLet’s try to make some predictions for the full model. We’ll use the mean body temperature we found earlier\n\nnew_points <- expand.grid(BodyTemp = 98.94, \n                          Weakness = c(\"None\", \"Mild\", \"Moderate\", \"Severe\"))\n\n##Mean prediction\nmean_pred <- predict(lm_mod, new_data = new_points)\nmean_pred\n\n# A tibble: 4 × 1\n  .pred\n  <dbl>\n1  98.6\n2  98.9\n3  98.9\n4  99.2\n\n#Confidence intervals\nconf_int_pred <- predict(lm_mod, \n                         new_data = new_points, \n                         type = \"conf_int\")\nconf_int_pred\n\n# A tibble: 4 × 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1        98.3        98.9\n2        98.7        99.0\n3        98.8        99.1\n4        99.0        99.4\n\n\n\nplot_data <- \n  new_points %>% \n  bind_cols(mean_pred) %>% \n  bind_cols(conf_int_pred)\n\nggplot(plot_data, aes(x = Weakness)) + \n  geom_point(aes(y = .pred)) + \n  geom_errorbar(aes(ymin = .pred_lower, \n                    ymax = .pred_upper),\n                width = .2) + \n  labs(y = \"Body Temperature (F)\")\n\n\n\n\nFitting univariate model - Logistic regression\n\nlog_mod <- logistic_reg() %>% \n  set_engine(\"glm\") %>% \n  fit(Nausea ~ Myalgia, data = flu)\n\ntidy(log_mod)\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic     p.value\n  <chr>              <dbl>     <dbl>     <dbl>       <dbl>\n1 (Intercept)       -1.37      0.280    -4.90  0.000000980\n2 MyalgiaMild        0.291     0.321     0.905 0.366      \n3 MyalgiaModerate    0.926     0.302     3.07  0.00217    \n4 MyalgiaSevere      1.42      0.337     4.22  0.0000244  \n\n#Results\nglance(log_mod)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          945.     729  -456.  920.  939.     912.         726   730\n\n\nFitting the full model - logistic regression\n\nlog_full <- logistic_reg() %>% \n  set_engine(\"glm\") %>% \n  fit(Nausea ~ ., data = flu)\n\ntidy(log_full)\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)             0.223     7.83     0.0285  0.977 \n 2 SwollenLymphNodesYes   -0.251     0.196   -1.28    0.200 \n 3 ChestCongestionYes      0.276     0.213    1.30    0.195 \n 4 ChillsSweatsYes         0.274     0.288    0.952   0.341 \n 5 NasalCongestionYes      0.426     0.255    1.67    0.0944\n 6 CoughYNYes             -0.140     0.519   -0.271   0.787 \n 7 SneezeYes               0.177     0.210    0.840   0.401 \n 8 FatigueYes              0.229     0.372    0.616   0.538 \n 9 SubjectiveFeverYes      0.278     0.225    1.23    0.218 \n10 HeadacheYes             0.331     0.285    1.16    0.245 \n# … with 28 more rows\n\nglance(log_full)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          945.     729  -376.  821.  982.     751.         695   730\n\n\nComparing model performance for univariate vs multiple logistic regression model.\n\ncompare_performance(log_mod, log_full)\n\n# Comparison of Model Performance Indices\n\nName     | Model | AIC (weights) | AICc (weights) | BIC (weights) | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n----------------------------------------------------------------------------------------------------------------------------------------------\nlog_mod  |  _glm | 920.3 (<.001) |  920.3 (<.001) | 938.7 (>.999) |     0.044 | 0.466 | 1.121 |    0.625 |  -110.929 |           0.006 | 0.565\nlog_full |  _glm | 821.5 (>.999) |  825.1 (>.999) | 982.2 (<.001) |     0.247 | 0.414 | 1.040 |    0.515 |      -Inf |           0.002 | 0.658\n\n\nThe full model appears to be a better fit than the univariate model, with an lower AIC of 821.5."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html",
    "href": "fluanalysis/code/modeleval.html",
    "title": "Model Evaluation",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nflu <- readRDS(here(\"fluanalysis/data/processed_data/flu.rds\"))\n\n\n# setting the seed \nset.seed(222)\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(flu, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\n\nn_flu <- \n  recipe(Nausea ~ ., data = train_data) \n\nSetting logistic regression engine\n\nlr_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\n\nCreating workflow function using logistic regression model and training data\n\nflu_wflow <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(n_flu)\n\nCreating function that can be used to prepare the recipe and train the model\n\nflu_fit <- \n  flu_wflow %>% \n  fit(data = train_data)\n\nExtracting the model objects from the workflow\n\nflu_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)             1.63      9.40      0.173  0.862 \n 2 SwollenLymphNodesYes   -0.241     0.232    -1.04   0.298 \n 3 ChestCongestionYes      0.219     0.257     0.853  0.394 \n 4 ChillsSweatsYes         0.115     0.332     0.346  0.729 \n 5 NasalCongestionYes      0.560     0.311     1.80   0.0713\n 6 CoughYNYes             -0.705     0.611    -1.15   0.249 \n 7 SneezeYes               0.117     0.248     0.473  0.636 \n 8 FatigueYes              0.177     0.438     0.403  0.687 \n 9 SubjectiveFeverYes      0.229     0.264     0.868  0.385 \n10 HeadacheYes             0.435     0.352     1.24   0.216 \n# … with 28 more rows\n\n\nUsing trained workflow to make predictions on testing data\n\npredict(flu_fit, test_data)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 Yes        \n 7 Yes        \n 8 No         \n 9 No         \n10 Yes        \n# … with 173 more rows\n\nflu_aug <- \n  augment(flu_fit, test_data)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n# The data look like: \nflu_aug %>%\n  select(Nausea, .pred_No, .pred_Yes)\n\n# A tibble: 183 × 3\n   Nausea .pred_No .pred_Yes\n   <fct>     <dbl>     <dbl>\n 1 No        0.962    0.0377\n 2 Yes       0.717    0.283 \n 3 No        0.680    0.320 \n 4 Yes       0.558    0.442 \n 5 No        0.830    0.170 \n 6 Yes       0.188    0.812 \n 7 Yes       0.254    0.746 \n 8 No        0.725    0.275 \n 9 No        0.720    0.280 \n10 Yes       0.281    0.719 \n# … with 173 more rows\n\n\nPlotting ROC_AUC\n\nflu_aug %>% \n  roc_curve(truth = Nausea, .pred_No) %>% \n  autoplot()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at <https://github.com/tidymodels/yardstick/issues>.\n\n\n\n\nflu_aug %>% \n  roc_auc(truth = Nausea, .pred_No) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.724\n\n\nAlternative model\n\nflu_r <- \n  recipe(Nausea ~ RunnyNose, data = train_data) \n\nCreating workflow function using lr model function and training data\n\nflu_wflow_r <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(flu_r)\n\nCreating function that can be used to prepare the recipe and train the model\n\nflu_fit_r <- \n  flu_wflow_r %>% \n  fit(data = train_data)\n\nExtracting the model objects from the workflow\n\nflu_fit_r %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic    p.value\n  <chr>           <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)    -0.790     0.172    -4.59  0.00000447\n2 RunnyNoseYes    0.188     0.202     0.930 0.352     \n\n\nUsing trained workflow to make predictions on testing data\n\npredict(flu_fit_r, test_data)\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 No         \n# … with 173 more rows\n\nflu_aug_r <- \n  augment(flu_fit_r, test_data)\n\n# The data look like: \nflu_aug_r %>%\n  select(Nausea, .pred_No, .pred_Yes)\n\n# A tibble: 183 × 3\n   Nausea .pred_No .pred_Yes\n   <fct>     <dbl>     <dbl>\n 1 No        0.688     0.312\n 2 Yes       0.646     0.354\n 3 No        0.646     0.354\n 4 Yes       0.646     0.354\n 5 No        0.688     0.312\n 6 Yes       0.688     0.312\n 7 Yes       0.646     0.354\n 8 No        0.688     0.312\n 9 No        0.688     0.312\n10 Yes       0.688     0.312\n# … with 173 more rows\n\n\nPlotting ROC_AUC\n\nflu_aug_r %>% \n  roc_curve(truth = Nausea, .pred_No) %>% \n  autoplot()\n\n\n\nflu_aug_r %>% \n  roc_auc(truth = Nausea, .pred_No) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.466\n\n\nThe model using only runny nose as a predictor for nausea performed much more poorly than the full model\n\nThis section added by KATIE WELLS\nModel Evaluation Part 2\n\nBodyTemp with all predictors\n\n# setting the seed \nset.seed(333)\n# Put 3/4 of the data into the training set \ndata_split2 <- initial_split(flu, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data2 <- training(data_split2)\ntest_data2  <- testing(data_split2)\n\nCreating recipe\n\nn_flu_bt <- \n  recipe(BodyTemp ~ ., data = train_data2) \n\nSetting engine\n\nlr_mod2 <- linear_reg() %>%\n  set_engine(\"lm\")\n\nCreating workflow function using linear regression model and training data\n\nflu_wflow_bt <- \n  workflow() %>% \n  add_model(lr_mod2) %>% \n  add_recipe(n_flu_bt)\nflu_wflow_bt\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nCreating function that can be used to prepare the recipe and train the model\n\nflu_fit_bt <- \n  flu_wflow_bt %>% \n  fit(data = train_data2)\n\nExtracting the model objects from the workflow\n\nflu_fit_bt %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic   p.value\n   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)           97.8        0.339   289.    0        \n 2 SwollenLymphNodesYes  -0.267      0.103    -2.59  0.00996  \n 3 ChestCongestionYes     0.111      0.110     1.01  0.313    \n 4 ChillsSweatsYes        0.178      0.139     1.28  0.203    \n 5 NasalCongestionYes    -0.119      0.129    -0.923 0.356    \n 6 CoughYNYes             0.331      0.296     1.12  0.264    \n 7 SneezeYes             -0.462      0.112    -4.14  0.0000400\n 8 FatigueYes             0.394      0.180     2.19  0.0287   \n 9 SubjectiveFeverYes     0.399      0.116     3.45  0.000615 \n10 HeadacheYes           -0.0632     0.144    -0.439 0.661    \n# … with 28 more rows\n\n\nUsing trained workflow to make predictions on testing data\n\npredict(flu_fit_bt, test_data2)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 183 × 1\n   .pred\n   <dbl>\n 1  98.9\n 2  99.6\n 3  99.0\n 4  98.8\n 5  98.8\n 6  98.3\n 7  99.4\n 8  99.3\n 9  99.6\n10  99.0\n# … with 173 more rows\n\nflu_aug_bt <- \n  augment(flu_fit_bt, test_data2)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n\n# The data look like: \nflu_aug_bt %>%\n  select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     97.8  98.9\n 2     98.2  99.6\n 3     99    99.0\n 4     99.2  98.8\n 5     98.5  98.8\n 6     98.2  98.3\n 7    100.   99.4\n 8     97.8  99.3\n 9     99.5  99.6\n10     99.7  99.0\n# … with 173 more rows\n\n\nRMSE\n\nflu_aug_bt %>% \n  rmse(truth = BodyTemp, .pred) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.27\n\n\n\n\nBodyTemp with main predictor RunnyNose\nCreating recipe\n\nn_flu_bt_rn <- \n  recipe(BodyTemp ~ RunnyNose, data = train_data2) \n\nCreating workflow function using linear model function and training data\n\nflu_wflow_bt_rn <- \n  workflow() %>% \n  add_model(lr_mod2) %>% \n  add_recipe(n_flu_bt_rn)\n\nCreating function that can be used to prepare the recipe and train the model\n\nflu_fit_bt_rn <- \n  flu_wflow_bt_rn %>% \n  fit(data = train_data2)\n\nExtracting the model objects from the workflow\n\nflu_fit_bt_rn %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    99.0      0.0913   1085.    0     \n2 RunnyNoseYes   -0.188    0.109      -1.73  0.0846\n\n\nUsing trained workflow to make predictions on testing data\n\npredict(flu_fit_bt_rn, test_data2)\n\n# A tibble: 183 × 1\n   .pred\n   <dbl>\n 1  98.9\n 2  99.0\n 3  98.9\n 4  98.9\n 5  98.9\n 6  98.9\n 7  98.9\n 8  99.0\n 9  99.0\n10  99.0\n# … with 173 more rows\n\nflu_aug_bt_rn <- \n  augment(flu_fit_bt_rn, test_data2)\n\n# The data look like: \nflu_aug_bt_rn %>%\n  select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     97.8  98.9\n 2     98.2  99.0\n 3     99    98.9\n 4     99.2  98.9\n 5     98.5  98.9\n 6     98.2  98.9\n 7    100.   98.9\n 8     97.8  99.0\n 9     99.5  99.0\n10     99.7  99.0\n# … with 173 more rows\n\n\nRMSE\n\nflu_aug_bt_rn %>%\n  rmse(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.27\n\n\nThe model with all predictors has a lower RMSE.\n\n\n\nThis above section added by KATIE WELLS"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html",
    "href": "fluanalysis/code/machinelearning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nImport lightly processed code…\n\nflu <- readRDS(here(\"fluanalysis/data/processed_data/flu.rds\"))\n\n#Feature/Variable removal Several symptoms exist within this dataset as a severity score and as Yes/No, and there is a duplicate for CoughYN… Fortunately for use, the name system of variables in this dataset makes this easy to achieve.\n\nflu <- flu %>% \n  select(-ends_with(\"YN\"), -matches(\"[0-9]\"))\n\n#Categorical/Ordinal predictors\n##the step below did not work flu_rec %>% step_dummy(Weakness, CoughIntensity, Myalgia) %>% step_ordinalscore()\n\nsev_score <- c(\"None\", \"Mild\", \"Moderate\", \"Severe\")\n\n#Low (“near-zero”) variance predictors\n\n## Creating subset of binary predictors\nbinary_vars <- flu %>%\n  select_if(~ is.factor(.) && nlevels(.) == 2)\n\n## Setting up logical vector where predictors have less than 50 entries equal 1.\nbinary_vars_tab <- binary_vars %>%\n  summarise_all(~ sum(table(.) < 50))\nlogi_vec <- binary_vars_tab == 1\n\n## Use which to find the indices with 'TRUE' in the logical vector\nindices <- which(logi_vec)\n\n## ... extracting the names of the predictors\nremove_vars <- names(binary_vars_tab[indices])\n##Vision and hearing should be removed...\n\n# And removing the identified binary predictors \nflu <- flu %>%\n  select(-all_of(remove_vars))\n\nNow that the dataset has been processed a bit more, let’s move on to the setting up our model.\n#Analysis code\nNext, we’ll split the testing and training data\n\n## setting the seed \nset.seed(123)\n## Put 3/4 of the data into the training set \ndata_split <- initial_split(flu, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n5x5 cross-validation\n\nset.seed(123)\nfive_fold <- vfold_cv(train_data, v = 5, strata = BodyTemp)\n\nSet the recipe\n\nflu_rec <- recipe(BodyTemp ~ ., data = train_data) %>% \n  step_dummy(all_predictors())\n\nThen we’ll set up a null model\n\nnull_mod <- null_model() %>% \n  set_engine(\"parsnip\") %>% \n  set_mode(\"regression\") %>% \n  translate()\n\nNull workflow\n\nnull_wflow <- workflow() %>%\n  add_model(null_mod) %>%\n  add_recipe(flu_rec)\n\n\nnull_fit <- null_wflow %>% \n  fit(data=train_data)\n\nnull_fit %>%\n  extract_fit_parsnip() %>%\n  tidy()\n\n# A tibble: 1 × 1\n  value\n  <dbl>\n1  99.0\n\n\nMean body temp is 98.97\n\nnull_aug <- augment(null_fit, train_data) \n\nnull_aug %>% \n  select(BodyTemp, .pred) %>%\n  rmse(BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.22\n\n\nTree\n\nlibrary(rpart)\n\n\nAttaching package: 'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\ntune_spec\n\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\ntree_grid\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             <dbl>      <int>\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# … with 15 more rows\n\ntree_grid %>% \n  count(tree_depth)\n\n# A tibble: 5 × 2\n  tree_depth     n\n       <int> <int>\n1          1     5\n2          4     5\n3          8     5\n4         11     5\n5         15     5\n\n\n\n## workflow for decision tree\ntree_wf <- workflow() %>%\n  add_model(tune_spec) %>%\n  add_recipe(flu_rec)\n\ntree_res <- \n  tree_wf %>% \n  tune_grid(resamples = five_fold,\n    grid = tree_grid)\n\n! Fold1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n\ntree_res %>% \n  collect_metrics() \n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n std_err .config \n             <dbl>      <int> <chr>   <chr>         <dbl> <int>   <dbl> <chr>   \n 1    0.0000000001          1 rmse    standard     1.19       5  0.0293 Preproc…\n 2    0.0000000001          1 rsq     standard     0.0578     5  0.0137 Preproc…\n 3    0.0000000178          1 rmse    standard     1.19       5  0.0293 Preproc…\n 4    0.0000000178          1 rsq     standard     0.0578     5  0.0137 Preproc…\n 5    0.00000316            1 rmse    standard     1.19       5  0.0293 Preproc…\n 6    0.00000316            1 rsq     standard     0.0578     5  0.0137 Preproc…\n 7    0.000562              1 rmse    standard     1.19       5  0.0293 Preproc…\n 8    0.000562              1 rsq     standard     0.0578     5  0.0137 Preproc…\n 9    0.1                   1 rmse    standard     1.22       5  0.0295 Preproc…\n10    0.1                   1 rsq     standard   NaN          0 NA      Preproc…\n# … with 40 more rows\n\n#visualize\ntree_res %>%\n  autoplot()\n\n\n\n#Selecting the best tree using rmse.\nbest_tree <- tree_res %>%\n  select_best(metric = \"rmse\") \n\nbest_tree \n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001          1 Preprocessor1_Model01\n\n\n\ntree_ff_wf <- tree_wf %>% \n  finalize_workflow(best_tree)\n\ntree_ff_wf %>% \n  fit(train_data)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 547 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 547 817.5396 98.97294  \n  2) SubjectiveFever_Yes< 0.5 166 113.3884 98.53735 *\n  3) SubjectiveFever_Yes>=0.5 381 658.9308 99.16273 *\n\n\n#Lasso\n\n##Setting up recipe.\nflu_rec_lasso <- recipe(BodyTemp ~ ., data = train_data) %>% \n  step_dummy(all_predictors())\n  \nlasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%\n  set_engine(\"glmnet\")\n\n#Setting work flow\nflu_wflow <- workflow() %>%\n  add_recipe(flu_rec_lasso)\n\nflu_lasso_fit <- flu_wflow %>%\n  add_model(lasso_spec) %>%\n  fit(data = train_data)\n\nflu_lasso_fit %>%\n  pull_workflow_fit() %>%\n  tidy()\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nℹ Please use `extract_fit_parsnip()` instead.\n\n\nWarning: package 'glmnet' was built under R version 4.2.3\n\n\nLoading required package: Matrix\n\n\nWarning: package 'Matrix' was built under R version 4.2.2\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-7\n\n\n# A tibble: 32 × 3\n   term                  estimate penalty\n   <chr>                    <dbl>   <dbl>\n 1 (Intercept)           98.7         0.1\n 2 SwollenLymphNodes_Yes  0           0.1\n 3 ChestCongestion_Yes    0           0.1\n 4 ChillsSweats_Yes       0.0474      0.1\n 5 NasalCongestion_Yes   -0.00553     0.1\n 6 Sneeze_Yes            -0.216       0.1\n 7 Fatigue_Yes            0.0304      0.1\n 8 SubjectiveFever_Yes    0.392       0.1\n 9 Headache_Yes           0           0.1\n10 Weakness_Mild          0           0.1\n# … with 22 more rows\n\n\n\n##Tune Lasso parameters\ntune_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n\nlambda_grid <- grid_regular(penalty(), levels = 50)\n\n\n# tune the grid using our workflow object.\ndoParallel::registerDoParallel()\n\nset.seed(2020)\nlasso_grid <- tune_grid(\n  flu_wflow %>% add_model(tune_spec),\n  resamples = five_fold,\n  grid = lambda_grid)\n\nLet take a look at the results\n\nlasso_grid %>%\n  collect_metrics()\n\n# A tibble: 100 × 7\n    penalty .metric .estimator   mean     n std_err .config              \n      <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 rmse    standard   1.22       5 0.0385  Preprocessor1_Model01\n 2 1   e-10 rsq     standard   0.0420     5 0.00979 Preprocessor1_Model01\n 3 1.60e-10 rmse    standard   1.22       5 0.0385  Preprocessor1_Model02\n 4 1.60e-10 rsq     standard   0.0420     5 0.00979 Preprocessor1_Model02\n 5 2.56e-10 rmse    standard   1.22       5 0.0385  Preprocessor1_Model03\n 6 2.56e-10 rsq     standard   0.0420     5 0.00979 Preprocessor1_Model03\n 7 4.09e-10 rmse    standard   1.22       5 0.0385  Preprocessor1_Model04\n 8 4.09e-10 rsq     standard   0.0420     5 0.00979 Preprocessor1_Model04\n 9 6.55e-10 rmse    standard   1.22       5 0.0385  Preprocessor1_Model05\n10 6.55e-10 rsq     standard   0.0420     5 0.00979 Preprocessor1_Model05\n# … with 90 more rows\n\nlasso_grid %>% \n  autoplot\n\n\n\n\n\n#Let's find the parameter with the lowest rmse\nlowest_rmse <- lasso_grid %>%\n  select_best(\"rmse\")\n\nfinal_lasso_wf <- finalize_workflow(\n  flu_wflow %>% add_model(tune_spec),\n  lowest_rmse)\n\n\nlasso_ff <- final_lasso_wf %>%\n  fit(train_data)\nlasso_ff \n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev   Lambda\n1   0  0.00 0.287500\n2   1  0.94 0.262000\n3   1  1.72 0.238700\n4   1  2.37 0.217500\n5   2  3.38 0.198200\n6   2  4.27 0.180600\n7   2  5.01 0.164500\n8   2  5.62 0.149900\n9   2  6.12 0.136600\n10  3  6.59 0.124500\n11  5  7.23 0.113400\n12  6  7.89 0.103300\n13  8  8.54 0.094150\n14  8  9.16 0.085790\n15  9  9.68 0.078170\n16  9 10.19 0.071220\n17 11 10.63 0.064890\n18 11 11.08 0.059130\n19 12 11.45 0.053880\n20 15 11.83 0.049090\n21 15 12.15 0.044730\n22 16 12.43 0.040760\n23 17 12.69 0.037140\n24 19 12.93 0.033840\n25 19 13.18 0.030830\n26 20 13.40 0.028090\n27 22 13.59 0.025600\n28 22 13.76 0.023320\n29 23 13.90 0.021250\n30 23 14.02 0.019360\n31 24 14.12 0.017640\n32 24 14.21 0.016070\n33 24 14.29 0.014650\n34 24 14.35 0.013350\n35 25 14.40 0.012160\n36 26 14.46 0.011080\n37 26 14.51 0.010100\n38 26 14.55 0.009199\n39 27 14.59 0.008381\n40 27 14.62 0.007637\n41 28 14.64 0.006958\n42 28 14.67 0.006340\n43 28 14.69 0.005777\n44 27 14.70 0.005264\n45 28 14.72 0.004796\n46 28 14.73 0.004370\n\n...\nand 31 more lines.\n\n\n\n##Let look at the most important predictors\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.2.3\n\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlasso_ff %>%\n  pull_workflow_fit() %>%\n  vi(lambda = lowest_rmse$penalty) %>%\n  mutate(\n    Importance = abs(Importance),\n    Variable = fct_reorder(Variable, Importance)\n  ) %>%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = NULL)\n\n\n\n\nWe can see that subjective fever is the most important predictor of body temperature and sneezing negatively correlated with body temperature.\n##Random forest model\n\nrf_spec <- rand_forest(min_n = tune(),  \n                       trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nrf_spec\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = tune()\n\nComputational engine: ranger \n\n\nCreate tuning workflow for random forest model\n\ntune_wf <- workflow() %>%\n  add_recipe(flu_rec) %>%\n  add_model(rf_spec)\n\n\ndoParallel::registerDoParallel()\n\nset.seed(345)\ntune_res <- tune_grid(\n  tune_wf,\n  resamples = five_fold,\n  grid = 20\n)\n\n#Visualize..\ntune_res %>% \n  autoplot()\n\n\n\nbest_rf <-   tune_res %>% \n  select_best(metric = \"rmse\")\n\nrf_final_wf <- tune_wf %>%  finalize_workflow(best_rf)\n\nrf_final_wf %>% \n  fit(data = train_data)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      min.node.size = min_rows(~37L, x), num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      547 \nNumber of independent variables:  31 \nMtry:                             5 \nTarget node size:                 37 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1.389161 \nR squared (OOB):                  0.07223827 \n\n\nFitting the last model to testing data\n\nset.seed(345)\nrf_final_fit <-\n  rf_final_wf %>%\n  last_fit(data_split)\n\nWarning: package 'ranger' was built under R version 4.2.3\n\nrf_final_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      1.12   Preprocessor1_Model1\n2 rsq     standard      0.0119 Preprocessor1_Model1"
  },
  {
    "objectID": "tidytuesday_exercise2.html",
    "href": "tidytuesday_exercise2.html",
    "title": "Tidy Tuesday Exercise 2",
    "section": "",
    "text": "Let’s load the data and other packages. We’ll use the tidytuesdayR package to do this.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\n\nWarning: package 'readr' was built under R version 4.2.2\n\n\nWarning: package 'purrr' was built under R version 4.2.2\n\n\nWarning: package 'dplyr' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\nWarning: package 'forcats' was built under R version 4.2.2\n\n\nWarning: package 'lubridate' was built under R version 4.2.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.0\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.2.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.4     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.4     ✔ yardstick    1.1.0\n✔ recipes      1.0.5     \n\n\nWarning: package 'broom' was built under R version 4.2.2\n\n\nWarning: package 'dials' was built under R version 4.2.2\n\n\nWarning: package 'scales' was built under R version 4.2.2\n\n\nWarning: package 'infer' was built under R version 4.2.2\n\n\nWarning: package 'modeldata' was built under R version 4.2.2\n\n\nWarning: package 'parsnip' was built under R version 4.2.2\n\n\nWarning: package 'recipes' was built under R version 4.2.2\n\n\nWarning: package 'rsample' was built under R version 4.2.2\n\n\nWarning: package 'tune' was built under R version 4.2.2\n\n\nWarning: package 'workflows' was built under R version 4.2.2\n\n\nWarning: package 'workflowsets' was built under R version 4.2.2\n\n\nWarning: package 'yardstick' was built under R version 4.2.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(tidytuesdayR)\n\nWarning: package 'tidytuesdayR' was built under R version 4.2.2\n\ntuesdata <- tidytuesdayR::tt_load('2023-04-11')\n\n--- Compiling #TidyTuesday Information for 2023-04-11 ----\n--- There are 2 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `egg-production.csv`\n    Downloading file 2 of 2: `cage-free-percentages.csv`\n\n\n--- Download complete ---\n\neggproduction <- tuesdata$`egg-production`\ncagefreepercentages <- tuesdata$`cage-free-percentages`\n\nI’ve copied the following data dictionary from the TiduTuesday GitHub to reference more easily.\n\nData Dictionary\n\n\negg-production.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nobserved_month\ndouble\nMonth in which report observations are collected,Dates are recorded in ISO 8601 format YYYY-MM-DD\n\n\nprod_type\ncharacter\ntype of egg product: hatching, table eggs\n\n\nprod_process\ncharacter\ntype of production process and housing: cage-free (organic), cage-free (non-organic), all. The value ‘all’ includes cage-free and conventional housing.\n\n\nn_hens\ndouble\nnumber of eggs produced by hens for a given month-type-process combo\n\n\nn_eggs\ndouble\nnumber of hens producing eggs for a given month-type-process combo\n\n\nsource\ncharacter\nOriginal USDA report from which data are sourced. Values correspond to titles of PDF reports. Date of report is included in title.\n\n\n\n\n\ncage-free-percentages.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nobserved_month\ndouble\nMonth in which report observations are collected,Dates are recorded in ISO 8601 format YYYY-MM-DD\n\n\npercent_hens\ndouble\nobserved or computed percentage of cage-free hens relative to all table-egg-laying hens\n\n\npercent_eggs\ndouble\ncomputed percentage of cage-free eggs relative to all table eggs,This variable is not available for data sourced from the Egg Markets Overview report\n\n\nsource\ncharacter\nOriginal USDA report from which data are sourced. Values correspond to titles of PDF reports. Date of report is included in title.\n\n\n\nLet’s start be taking a look at the data\n\nglimpse(eggproduction)\n\nRows: 220\nColumns: 6\n$ observed_month <date> 2016-07-31, 2016-08-31, 2016-09-30, 2016-10-31, 2016-1…\n$ prod_type      <chr> \"hatching eggs\", \"hatching eggs\", \"hatching eggs\", \"hat…\n$ prod_process   <chr> \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\",…\n$ n_hens         <dbl> 57975000, 57595000, 57161000, 56857000, 57116000, 57750…\n$ n_eggs         <dbl> 1147000000, 1142700000, 1093300000, 1126700000, 1096600…\n$ source         <chr> \"ChicEggs-09-23-2016.pdf\", \"ChicEggs-10-21-2016.pdf\", \"…\n\nglimpse(cagefreepercentages)\n\nRows: 96\nColumns: 4\n$ observed_month <date> 2007-12-31, 2008-12-31, 2009-12-31, 2010-12-31, 2011-1…\n$ percent_hens   <dbl> 3.20000, 3.50000, 3.60000, 4.40000, 5.40000, 6.00000, 5…\n$ percent_eggs   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 9.634938, NA, 9…\n$ source         <chr> \"Egg-Markets-Overview-2019-10-19.pdf\", \"Egg-Markets-Ove…\n\n\nIt seems like these datasets span a different range of years. Let’s verify this.\n\neggproduction %>% \n  arrange(observed_month) %>%\n  slice(c(1, n())) %>%\n  bind_rows()\n\n# A tibble: 2 × 6\n  observed_month prod_type     prod_process          n_hens     n_eggs source   \n  <date>         <chr>         <chr>                  <dbl>      <dbl> <chr>    \n1 2016-07-31     hatching eggs all                 57975000 1147000000 ChicEggs…\n2 2021-02-28     table eggs    cage-free (organic) 17491500  386912160 PY202103…\n\ncagefreepercentages %>% \n  arrange(observed_month) %>%\n  slice(c(1, n())) %>%\n  bind_rows()\n\n# A tibble: 2 × 4\n  observed_month percent_hens percent_eggs source                             \n  <date>                <dbl>        <dbl> <chr>                              \n1 2007-12-31              3.2           NA Egg-Markets-Overview-2019-10-19.pdf\n2 2021-02-28             29.2           NA Egg-Markets-Overview-2021-03-05.pdf\n\n\nThe cage-free percentages data goes back to 2007, while the egg production data only goes back to 2016. The most recent observations in both datasets is Feb. 28, 2021.\nMy data with the hypothesis that the number of eggs produced per hen differs based on the type of egg product (hatchling or table eggs).\nH0: There is no difference between the number of eggs produced per hen. HA: There is a difference between the number of eggs produced per hen, and table eggs are produced at higher rates compared to hatchling eggs.\nThe outcome interest is the rate of egg production, which I will name rate_prod for rate of production.\n\neggproduction <- eggproduction %>% \n  group_by(prod_type) %>% \n  mutate(rate_prod  = n_eggs/n_hens)\n\nLet’s prepare the datasets for a merge, which will hopefully this will make things easy to work with. I’ll start by making a variable that pulls just the month and year from ‘observed_month’ to make merge the datasets more seamless.\n\ncagefreepercentages <- cagefreepercentages %>% \n  mutate(mon_yr = format_ISO8601(observed_month, precision = \"ym\"),\n         mon = format(observed_month, \"%m\"),\n        mon = as.factor(mon)) #only month\n\neggproduction <- eggproduction %>% \n  mutate(mon_yr = format_ISO8601(observed_month, precision = \"ym\"), \n         mon = format(observed_month, \"%m\"),\n        mon = as.factor(mon))\n\nI’ll do a bit more tidying up by using the computed estimates and by removing unnecessary variables\n\ncagefreepercentages_clean <- cagefreepercentages %>% \n  filter(source == \"computed\")%>% \n  select(-c(observed_month, source))\n\neggproduction_clean <- eggproduction %>% \n  select(-c(observed_month, source))\n\nNow, we’ll use inner join to merge by the mon_yr only that appear in both datasets\n\neggs_df <- inner_join(eggproduction_clean, cagefreepercentages_clean, by = join_by(mon_yr, mon))\n\neggs_df <- eggs_df %>% \n  mutate(prod_type = as.factor(prod_type),\n         prod_process = as.factor(prod_process))\n\neggs_df$prod_type <- relevel(eggs_df$prod_type, ref = \"table eggs\")\n\n#eggs_clean <- eggs_df %>% select(-mon_yr)\n\nLet’s split the data into test and train\n\n## setting the seed \nset.seed(123)\n## Put 3/4 of the data into the training set \ndata_split <- initial_split(eggs_df, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\negg_metrics <- metric_set(accuracy, roc_auc, mn_log_loss)\n\n#Cross validation\nset.seed(123)\nfive_fold <- vfold_cv(train_data, v = 5, strata = prod_type)\n\nSet the recipe\n\neggs_rec <- recipe(prod_type ~ prod_process + mon + rate_prod, data = train_data) %>% \n  step_dummy(prod_process, mon)\n\nNow let’s set up a null model\n\nnull_mod <- null_model() %>% \n  set_engine(\"parsnip\") %>% \n  set_mode(\"classification\")\n\nNull workflow\n\nnull_wflow <- workflow() %>%\n  add_model(null_mod) %>%\n  add_recipe(eggs_rec)\n\n\nnull_fit <- null_wflow %>% \n  fit(data = train_data)\n\nnull_fit %>%\n  extract_fit_parsnip() %>%\n  tidy()\n\n# A tibble: 1 × 1\n  value     \n  <chr>     \n1 table eggs\n\n\nDecision Tree\n\nlibrary(rpart)\n\n\nAttaching package: 'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"classification\")\n\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\n\n## workflow for decision tree\ntree_wf <- workflow() %>%\n  add_model(tune_spec) %>%\n  add_recipe(eggs_rec)\n\ntree_res <- \n  tree_wf %>% \n  tune_grid(resamples = five_fold,\n    grid = tree_grid) \n\n\ntree_res %>% \n  collect_metrics() \n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>     \n 1    0.0000000001          1 accuracy binary         1     5       0 Preproces…\n 2    0.0000000001          1 roc_auc  binary         1     5       0 Preproces…\n 3    0.0000000178          1 accuracy binary         1     5       0 Preproces…\n 4    0.0000000178          1 roc_auc  binary         1     5       0 Preproces…\n 5    0.00000316            1 accuracy binary         1     5       0 Preproces…\n 6    0.00000316            1 roc_auc  binary         1     5       0 Preproces…\n 7    0.000562              1 accuracy binary         1     5       0 Preproces…\n 8    0.000562              1 roc_auc  binary         1     5       0 Preproces…\n 9    0.1                   1 accuracy binary         1     5       0 Preproces…\n10    0.1                   1 roc_auc  binary         1     5       0 Preproces…\n# … with 40 more rows\n\n#visualize\ntree_res %>%\n  autoplot()\n\n\n\n#Selecting the best tree using rmse.\nbest_tree <- tree_res %>%\n  select_best(metric = \"roc_auc\") \n\nI am unsure if I am interpretting this correctly, but it seems like accuracy and ROC_AUC were perfect?\nLogistic regression\n\nlog_spec <- logistic_reg() %>%\n  set_engine(engine = \"glm\") %>%\n  set_mode(\"classification\")\n\nCreating workflow object for logistic regression\n\nlog_wflow <- \n   workflow() %>% \n   add_recipe(eggs_rec) %>% \n   add_model(log_spec)\n\nCreating trained workflow\n\nlogreg_fit <- log_wflow %>% \n  fit(data = train_data)\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nPulling model fit\n\nlogreg_fit %>% \n  extract_fit_parsnip()  \n\nparsnip model object\n\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n                         (Intercept)                             rate_prod  \n                            237.5439                              -10.7688  \nprod_process_cage.free..non.organic.      prod_process_cage.free..organic.  \n                            -14.2449                              -14.2947  \n                             mon_X02                               mon_X03  \n                            -21.6212                               -0.8866  \n                             mon_X04                               mon_X05  \n                             -7.6954                                0.4962  \n                             mon_X06                               mon_X07  \n                             -7.1329                                0.3517  \n                             mon_X08                               mon_X09  \n                              1.8642                               -7.1652  \n                             mon_X10                               mon_X11  \n                              0.5256                               -5.8567  \n                             mon_X12  \n                              0.1296  \n\nDegrees of Freedom: 161 Total (i.e. Null);  147 Residual\nNull Deviance:      185.4 \nResidual Deviance: 1.26e-09     AIC: 30\n\n\nRandom forest\n\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.2.3\n\nrf_spec <- \n  rand_forest(min_n = tune()) %>% \n  set_engine(\"ranger\", importance = \"impurity\") %>% \n  set_mode(\"classification\")\n\nRandom forest workflow\n\nrf_wflow <- workflow() %>%\n add_recipe(eggs_rec) %>% \n add_model(rf_spec) \n\nFit a model\n\nrf_wflow_fit <- rf_wflow %>% \n  fit(data = train_data)\n\nWarning: tune samples were requested but there were 162 rows in the data. 162\nwill be used.\n\n\n\ndoParallel::registerDoParallel()\n\nset.seed(345)\ntune_res <- tune_grid(\n  rf_wflow,\n  resamples = five_fold,\n  grid = 20)\n\n#Visualize..\ntune_res %>% \n  autoplot()\n\n\n\nbest_rf <-   tune_res %>% \n  select_best(metric = \"roc_auc\")\n\nrf_final_wf <- rf_wflow %>%\n  finalize_workflow(best_rf)\n\nrf_final_wf %>% \n  fit(data = train_data)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, min.node.size = min_rows(~36L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      162 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 36 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.01356042 \n\n\nSame results as above…\nMultinomial regression via neural net\n\n# Specify a multinomial regression via nnet\nmultireg_spec <- multinom_reg(penalty = 1) %>% \n  set_engine(\"nnet\") %>% \n  set_mode(\"classification\")\n\n#trying to set workflow\nmultireg_wf <- workflow() %>% \n  add_model(multireg_spec) %>% \n  add_recipe(eggs_rec)\n\n# Train a multinomial regression model ##\nset.seed(2056)\nmultireg_fit <- multireg_spec %>% \n  fit(prod_type ~ prod_process + mon + rate_prod, data = train_data)\n  \n#print model\nmultireg_fit %>%\n  tidy()\n\n# A tibble: 15 × 6\n   y.level term                                estimate std.e…¹ statis…² p.value\n   <chr>   <chr>                                  <dbl>   <dbl>    <dbl>   <dbl>\n 1 1       (Intercept)                          2.37     1.95    1.21     0.225 \n 2 1       prod_processcage-free (non-organic) -1.54     0.659  -2.34     0.0192\n 3 1       prod_processcage-free (organic)     -1.64     0.639  -2.57     0.0101\n 4 1       mon02                               -0.00993  1.12   -0.00888  0.993 \n 5 1       mon03                                0.106    1.01    0.105    0.916 \n 6 1       mon04                                0.0334   0.998   0.0335   0.973 \n 7 1       mon05                                0.0692   0.998   0.0693   0.945 \n 8 1       mon06                                0.188    1.04    0.180    0.857 \n 9 1       mon07                                0.315    1.08    0.291    0.771 \n10 1       mon08                                0.340    0.933   0.365    0.715 \n11 1       mon09                                0.300    0.932   0.322    0.748 \n12 1       mon10                                0.0470   0.992   0.0474   0.962 \n13 1       mon11                                0.195    0.962   0.203    0.839 \n14 1       mon12                                0.234    0.964   0.242    0.809 \n15 1       rate_prod                           -0.144    0.0841 -1.71     0.0879\n# … with abbreviated variable names ¹​std.error, ²​statistic\n\n\nAll the models appears to perform similarly… the logistic regression model is the might be the simplest model, so let’s use that to to test model performance against the test_data\n\n# Make predictions for the test set\neggs_results <- test_data %>% \n  select(prod_type) %>% \n  bind_cols(logreg_fit %>% \n              predict(new_data = test_data)) %>% \n  bind_cols(logreg_fit %>% \n              predict(new_data = test_data, type = \"prob\"))\n\n# Print predictions\neggs_results %>% \n  slice_head(n = 54)\n\n# A tibble: 54 × 4\n# Groups:   prod_type [2]\n   prod_type  .pred_class `.pred_table eggs` `.pred_hatching eggs`\n   <fct>      <fct>                    <dbl>                 <dbl>\n 1 table eggs table eggs                1.00              1.62e-12\n 2 table eggs table eggs                1.00              1.82e-11\n 3 table eggs table eggs                1.00              3.74e-13\n 4 table eggs table eggs                1                 2.22e-16\n 5 table eggs table eggs                1.00              7.49e-10\n 6 table eggs table eggs                1.00              2.27e-12\n 7 table eggs table eggs                1                 2.22e-16\n 8 table eggs table eggs                1                 2.22e-16\n 9 table eggs table eggs                1.00              1.87e-12\n10 table eggs table eggs                1                 2.22e-16\n# … with 44 more rows\n\n\n\n#Confusion matrix\nconf_mat(eggs_results,\n         truth = prod_type,\n         estimate = .pred_class) \n\n# A tibble: 2 × 2\n  prod_type     conf_mat  \n  <fct>         <list>    \n1 table eggs    <conf_mat>\n2 hatching eggs <conf_mat>\n\n## Did not work like expected...\n\n#pulling accuracy\naccuracy(eggs_results, \n         truth = prod_type,\n         estimate = .pred_class)\n\n# A tibble: 2 × 4\n  prod_type     .metric  .estimator .estimate\n  <fct>         <chr>    <chr>          <dbl>\n1 table eggs    accuracy binary             1\n2 hatching eggs accuracy binary             1\n\n##Is 100% accurate\n\n## trying to pull ROC-AUC to see if performance of predictive model is good\nroc_auc(eggs_results,\n        truth = prod_type,\n        `.pred_hatching eggs`) \n\nWarning: There were 2 warnings in `dplyr::summarise()`.\nThe first warning was:\nℹ In argument: `.estimate = metric_fn(...)`.\nℹ In group 1: `prod_type = table eggs`.\nCaused by warning:\n! No control observations were detected in `truth` with control level 'hatching eggs'.\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n# A tibble: 2 × 4\n  prod_type     .metric .estimator .estimate\n  <fct>         <chr>   <chr>          <dbl>\n1 table eggs    roc_auc binary            NA\n2 hatching eggs roc_auc binary            NA\n\n##estimate is NA?\n\nAs far as I can tell, it seemed like the logistic regression model was able to predict prod_type perfectly. Intuitively, it seems like these models should not performed perfectly, but in looking at the data it is very clear that hens producing table eggs lay significantly more eggs than those producing hatching eggs. Unfortunately, my knowledge of tidymodels has limited my ability to troubleshoot potential issues in these classification models…"
  }
]